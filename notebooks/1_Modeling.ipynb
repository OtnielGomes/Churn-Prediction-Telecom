{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e0fdcbb-5814-4bd7-9323-99338b744b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Customer Churn - Telecom\n",
    "---\n",
    "\n",
    "### CRISP-DM Methodology\n",
    "This project follows the CRISP-DM (*Cross-Industry Standard Process for Data Mining*) framework applied to **Customer Retention & Churn Prediction**:\n",
    "| **Stage** | **Objective** | **Methodological Execution** |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. Business Understanding** | Mitigate revenue loss by identifying at-risk customers. | • **Target Definition**: Binary Classification (Churn: Yes/No).<br>• **KPIs**: Maximize **Lift** in retention campaigns & Revenue Saved vs. Cost. |\n",
    "| **2. Data Understanding** | Detect patterns of friction and dissatisfaction. | • **EDA**: Distribution analysis (Detect Imbalance).<br>• **Hypothesis Testing**: Correlation Matrix & Independence Tests (Chi-Square). |\n",
    "| **3. Data Preparation** | Construct a robust dataset for parametric modeling. | • **Scaling**: Standardization (Z-score) for coefficient comparability.<br>• **Encoding**: One-Hot Encoding for nominal variables.<br>• **Splitting**: Stratified Train/Test Split to preserve class ratio. |\n",
    "| **4. Modeling** | Estimate Churn Probability:<br>$$P(Y=1 \\vert X) = \\frac{1}{1+e^{-z}}$$ | • **Algorithm**: Logistic Regression (Baseline).<br>• **Inference**: Analyze **Odds Ratios** to determine feature elasticity. |\n",
    "| **5. Evaluation** | Assess model reliability and financial impact. | • **Discrimination**: AUC-ROC & F1-Score (Threshold Tuning).<br>• **Calibration**: Probability Calibration Curve (Reliability Diagram). |\n",
    "| **6. Deployment** | Integrate insights into the CRM lifecycle. | • **Deliverable**: \"High-Risk\" Customer List for Marketing Squad.<br>• **Artifact**: Serialize model (`joblib`) for batch inference. |\n",
    "\n",
    "---\n",
    "#### Note:\n",
    "\n",
    "Although the CRISP-DM Modeling phase typically involves comparing several algorithms to select the best performer, this project focuses, by scope definition, on implementing a baseline. Therefore, a Logistic Regression model will be developed, going through all stages of the cycle (analysis, preparation, and modeling) to validate the initial hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d932ccc9-9725-4c0b-a659-a24aa6f2dd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Installs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d3a5b4-e41a-4a61-bce4-4dceb05903fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install numpy==2.4.0\n",
    "%pip install pandas==2.3.3\n",
    "%pip install scikit-learn==1.8.0\n",
    "%pip install matplotlib==3.10.8\n",
    "%pip install seaborn==0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2c301e-10de-4584-a830-7fc5062aeede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Command to restart the kernel and update the installed libraries\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e9c6310-74b4-4ed9-a99e-8f757cc87e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff0c4ba1-0c69-492d-96f7-660939c88027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Analize and Visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Modeling / Model Linear / Metrics / Save Model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0423f5ec-298e-4812-a2d4-489b479224a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dev objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f94f823-143c-4818-95a3-55ce8c888e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================= #\n",
    "# >>> Module of functions and classes for creating graphs and visualizing data. #                                        \n",
    "# ============================================================================= #\n",
    "\n",
    "# ======================================================== #\n",
    "# Imports:                                                 #\n",
    "# ======================================================== #\n",
    "# Graphics:\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Seaborn\n",
    "import seaborn as sns\n",
    "# Data manipulation and visualization:\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "\n",
    "# ======================================================== #\n",
    "# Graphics Data - Class                                    #\n",
    "# ======================================================== #\n",
    "\n",
    "class GraphicsData:\n",
    "\n",
    "    # Initialize Class\n",
    "    def __init__(\n",
    "        self, \n",
    "        data : pd.DataFrame,\n",
    "\n",
    "    ):\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if data.empty:\n",
    "                raise ValueError('The provided DataFrame is empty.')\n",
    "\n",
    "            self.data = data\n",
    "\n",
    "        except Exception  as e:\n",
    "            print(f'[Error] Failed to load Dataframe : {str(e)}')\n",
    "\n",
    "    # ======================================================== #\n",
    "    # Initializer Subplot Grid - Function                      #\n",
    "    # ======================================================== #\n",
    "    def _initializer_subplot_grid(\n",
    "        self, \n",
    "        num_columns, \n",
    "        figsize_per_row,\n",
    "        h_size: int = 25\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes and returns a standardized matplotlib subplot grid layout.\n",
    "\n",
    "        This utility method calculates the required number of rows based on \n",
    "        the number of variables in the dataset and the desired number of \n",
    "        columns per row. It then creates a grid of subplots accordingly and \n",
    "        applies a consistent styling.\n",
    "\n",
    "        Args:\n",
    "            num_columns (int): Number of subplots per row.\n",
    "            figsize_per_row (int): Vertical size (height) per row in the final figure.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - fig (matplotlib.figure.Figure): The full matplotlib figure object.\n",
    "                - ax (np.ndarray of matplotlib.axes._subplots.AxesSubplot): Flattened array of subplot axes.\n",
    "        \"\"\"\n",
    "        num_vars = len(self.data.columns)\n",
    "        num_rows = (num_vars + num_columns - 1) // num_columns\n",
    "\n",
    "        plt.rc('font', size = 12)\n",
    "        fig, ax = plt.subplots(num_rows, num_columns, figsize = (h_size, num_rows * figsize_per_row))\n",
    "        ax = ax.flatten()\n",
    "        sns.set(style = 'whitegrid')\n",
    "\n",
    "        return fig, ax\n",
    "    \n",
    "    # ======================================================== #\n",
    "    # Finalize Subplot Layout - Function                       #\n",
    "    # ======================================================== #\n",
    "    def _finalize_subplot_layout(\n",
    "        self,\n",
    "        fig,\n",
    "        ax,\n",
    "        i: int,\n",
    "        title: str = None,\n",
    "        fontsize: int = 30,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Finalizes and displays a matplotlib figure by adjusting layout and removing unused subplots.\n",
    "\n",
    "        This method is used after plotting multiple subplots to:\n",
    "        - Remove any unused axes in the grid.\n",
    "        - Set a central title for the entire figure.\n",
    "        - Automatically adjust spacing and layout for better readability.\n",
    "        - Display the resulting plot.\n",
    "\n",
    "        Args:\n",
    "            fig (matplotlib.figure.Figure): The matplotlib figure object containing the subplots.\n",
    "            ax (np.ndarray of matplotlib.axes.Axes): Array of axes (flattened) for all subplots.\n",
    "            i (int): Index of the last used subplot (all subplots after this will be removed).\n",
    "            title (str, optional): Title to be displayed at the top of the entire figure.\n",
    "            fontsize (int, optional): Font size of the overall title. Default is 30.\n",
    "        \"\"\"\n",
    "        for j in range(i + 1, len(ax)):\n",
    "                fig.delaxes(ax[j])\n",
    "        \n",
    "        plt.suptitle(title, fontsize = fontsize, fontweight = 'bold')\n",
    "        plt.tight_layout(rect = [0, 0, 1, 0.97])\n",
    "        plt.show()\n",
    "\n",
    "    # ======================================================== #\n",
    "    # Format Single AX - Function                              #\n",
    "    # ======================================================== #\n",
    "    def _format_single_ax(\n",
    "        self,\n",
    "        ax,\n",
    "        title: str = None,\n",
    "        fontsize: int = 20,\n",
    "        linewidth: float = 0.9,\n",
    "        feature_skew: float = 0,\n",
    "        pct_outliers: float = 0\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Applies standard formatting to a single subplot axis.\n",
    "\n",
    "        This method configures a single axis by:\n",
    "        - Setting the title with specified font size and bold style.\n",
    "        - Hiding the x and y axis labels.\n",
    "        - Adding dashed grid lines for both axes with configurable line width.\n",
    "\n",
    "        Args:\n",
    "            ax (matplotlib.axes.Axes): The axis to be formatted.\n",
    "            title (str, optional): Title text for the axis. Defaults to None.\n",
    "            fontsize (int, optional): Font size for the title. Defaults to 20.\n",
    "            linewidth (float, optional): Width of the dashed grid lines. Defaults to 0.9.\n",
    "        \"\"\"\n",
    "        ax.set_title(title, fontsize = fontsize, fontweight = 'bold')\n",
    "        ax.set_xlabel(None)\n",
    "        ax.set_ylabel(None)\n",
    "        ax.grid(axis = 'y', which = 'major', linestyle = '--', linewidth = linewidth)\n",
    "        ax.grid(axis = 'x', which = 'major', linestyle = '--', linewidth = linewidth)\n",
    "        \n",
    "        # Visual maker of normality (Skewness = 0)\n",
    "        if abs(feature_skew) > 1:\n",
    "            ax.set_facecolor('#fdf2e9') # Light orange background to indicate high asymmetry\n",
    "        \n",
    "        # Visual maker of Outliers (Pct > 5%)\n",
    "        if pct_outliers > 5.0:\n",
    "            ax.set_facecolor('#fffbe6')\n",
    "      \n",
    "\n",
    "\n",
    "    def numerical_histograms(\n",
    "        self, \n",
    "        num_columns: int = 3,\n",
    "        figsize_per_row: int = 6,\n",
    "        color: str = '#3498db',\n",
    "        hue: str = None,\n",
    "        hue_order: list = None,\n",
    "        palette: list = ['#1abc9c', '#ff6b6b'] ,\n",
    "        title: str = 'Histograms of Numerical Variables',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots histograms with KDE (Kernel Density Estimation) for all numerical columns in the dataset.\n",
    "\n",
    "        Optionally groups the histograms by a categorical target variable using different colors (hue).\n",
    "        Useful for visualizing the distribution of numerical features and how they differ between groups.\n",
    "\n",
    "        Args:\n",
    "            num_columns (int): Number of plots per row in the subplot grid.\n",
    "            figsize_per_row (int): Height of each row in inches (controls vertical spacing).\n",
    "            color (str): Default color for histograms when `hue` is not specified.\n",
    "            hue (str, optional): Name of the column used for grouping (e.g., 'churn_target'). Must be categorical.\n",
    "            palette (list): List of colors for hue levels. Only used if `hue` is provided.\n",
    "            title (str): Title of the entire figure layout.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If plotting fails due to missing columns, incorrect types, or rendering errors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            # Order HUE\n",
    "            if hue:\n",
    "                hue_order = sorted(self.data[hue].dropna().unique())\n",
    "\n",
    "            # Entry checks\n",
    "            numeric_cols = self.data.select_dtypes(include = 'number').columns.tolist()\n",
    "            if hue and hue in numeric_cols:\n",
    "                numeric_cols.remove(hue)\n",
    "\n",
    "            # Define AX and Fig\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row)\n",
    "\n",
    "            \n",
    "            for i, column in enumerate(numeric_cols):\n",
    "\n",
    "                \n",
    "                # Calculate Skewness for the title\n",
    "                # dropna() is necessary for the statistical calculation not to fail\n",
    "                feature_skew = self.data[column].dropna().skew()\n",
    "                \n",
    "                sns.histplot(\n",
    "                    data = self.data,\n",
    "                    x = column,\n",
    "                    kde = True,\n",
    "                    hue = hue,\n",
    "                    hue_order = hue_order,\n",
    "                    palette = palette if hue else None,\n",
    "                    edgecolor = 'black',\n",
    "                    line_kws = {'linewidth': 2},\n",
    "                    alpha = 0.4 if hue else 0.7,\n",
    "                    color = None if hue else color,\n",
    "                    ax = ax[i],\n",
    "                )\n",
    "\n",
    "        \n",
    "                # Line Means for Group\n",
    "                if hue:\n",
    "\n",
    "                    for j, target_val in enumerate(hue_order):\n",
    "\n",
    "                        # Filter\n",
    "                        subset_data = self.data[self.data[hue] == target_val][column]\n",
    "                        mean_val = subset_data.mean()\n",
    "\n",
    "                        # Color \n",
    "                        line_color = palette[j] if palette and j < len(palette) else 'black'\n",
    "\n",
    "                        # Plot Line\n",
    "                        ax[i].axvline(\n",
    "                            mean_val,\n",
    "                            color = line_color,\n",
    "                            linestyle = '--',\n",
    "                            linewidth = 2.5,\n",
    "                            label = f'Mean({target_val}: {mean_val:.2f})'\n",
    "                        )\n",
    "                    ax[i].legend(fontsize = 10)\n",
    "\n",
    "                # Config Ax's\n",
    "                self._format_single_ax(ax[i], title = f'Variable: {column}. \\n Skewness: {feature_skew:.2f}', feature_skew = feature_skew)\n",
    "                \n",
    "\n",
    "                    \n",
    "            # Show Graphics\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)\n",
    "        except Exception as e:\n",
    "            print(f'[Error] Failed to generate numeric histograms: {str(e)}.')\n",
    "    \n",
    "\n",
    "    # ======================================================== #\n",
    "    # Numerical Boxplots - Function                            #\n",
    "    # ======================================================== #\n",
    "    def numerical_boxplots(\n",
    "        self, \n",
    "        hue: str = None, \n",
    "        num_columns: int = 3,\n",
    "        figsize_per_row: int = 6,\n",
    "        palette: list = ['#1abc9c', '#ff6b6b'],\n",
    "        color: str = '#1abc9c',\n",
    "        showfliers: bool = False,\n",
    "        showmeans: bool = False,\n",
    "        title: str = 'Boxplots of Numerical Variables',\n",
    "        legend: list = []\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots boxplots for each numerical variable in the dataset.\n",
    "\n",
    "        Optionally groups the boxplots by a categorical hue variable (e.g., churn target), \n",
    "        allowing for comparison of distributions between groups. Helps identify outliers, \n",
    "        skewness, and variability in each feature.\n",
    "\n",
    "        Args:\n",
    "            hue (str, optional): Column name to group the boxplots (e.g., 'churn_target').\n",
    "                                If None, individual boxplots are created without grouping.\n",
    "            num_columns (int): Number of plots per row in the subplot grid.\n",
    "            figsize_per_row (int): Height (in inches) of each row of plots.\n",
    "            palette (list): Color palette to use when `hue` is provided.\n",
    "            color (str): Single color to use when `hue` is not specified.\n",
    "            showfliers (bool): Whether to display outlier points in the boxplots (default: False).\n",
    "            title (str): Overall title for the subplot grid.\n",
    "            legend (list): Custom legend labels to replace default tick labels when `hue` is present.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the hue column is not found in the DataFrame.\n",
    "            Exception: If plotting fails due to unexpected issues.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if hue and hue not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{hue}' not in the DataFrame.\")\n",
    "\n",
    "            numeric_cols = self.data.select_dtypes(include = 'number').columns.tolist()\n",
    "            if hue and hue in numeric_cols:\n",
    "                numeric_cols.remove(hue)\n",
    "\n",
    "            # Define AX and Fig\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row, h_size = 25)\n",
    "\n",
    "            for i, column in enumerate(numeric_cols):\n",
    "\n",
    "                    data_col = self.data[column].dropna()\n",
    "\n",
    "                    # Calcule of Outliers (Rule of Tukey)\n",
    "                    Q1 = data_col.quantile(0.25)\n",
    "                    Q3 = data_col.quantile(0.75)\n",
    "                    median = data_col.quantile(0.50)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_fence = Q1 -1.5 * IQR\n",
    "                    upper_fence = Q3 + 1.5 * IQR\n",
    "\n",
    "                    # Count Outliers\n",
    "                    outliers = data_col[(data_col < lower_fence) | (data_col > upper_fence)]\n",
    "                    n_outliers = len(outliers)\n",
    "                    pct_outliers = (n_outliers / len(data_col)) * 100\n",
    "\n",
    "                    sns.boxplot(\n",
    "                        data = self.data,\n",
    "                        x = hue if hue else column,\n",
    "                        y = column if hue else None,\n",
    "                        hue = hue if hue else None,\n",
    "                        palette = palette if hue else None,\n",
    "                        color = None if hue else color,\n",
    "                        showfliers = showfliers,\n",
    "                        showmeans = showmeans,\n",
    "                        #orient = 'h',\n",
    "                        meanprops = {\"marker\": \"o\", \"markerfacecolor\": \"white\", \"markeredgecolor\": \"black\"},\n",
    "                        fliersize = 3,\n",
    "                        #legend = False,\n",
    "                        ax = ax[i]\n",
    "                    )\n",
    "\n",
    "                    # Config Ax's\n",
    "                    if len(legend) > 0:\n",
    "                        ax[i].set_xticks([l for l in range(0, len(legend))])\n",
    "                        ax[i].set_xticklabels(legend, fontsize = 14, fontweight = 'bold')\n",
    "                    \n",
    "                    if ax[i].get_legend():\n",
    "                        ax[i].legend_.remove()\n",
    "                    self._format_single_ax(ax[i], f'Variable: {column}\\nOutliers: {n_outliers} ({pct_outliers:.1f}%)', pct_outliers = pct_outliers) \n",
    "                    ax[i].set_yticklabels([])\n",
    "                    #sns.despine(ax = ax[i], top = True, right = True, left = True, bottom = True)\n",
    "            \n",
    "            # Show Graphics\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)\n",
    "        except Exception as e: \n",
    "            print(f'[ERROR] Failed to generate numerical boxplots: {str(e)}.')\n",
    "    \n",
    "    # ======================================================== #\n",
    "    # Categorical Countplots - Function                        #\n",
    "    # ======================================================== #\n",
    "    def categorical_countplots(\n",
    "        self,\n",
    "        hue: str = None,\n",
    "        num_columns: int = 3,\n",
    "        figsize_per_row: int = 7,\n",
    "        palette: list = ['#1abc9c', '#ff6b6b'],\n",
    "        color: str = '#8e44ad',\n",
    "        title: str = 'Countplots of Categorical Variables '\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots countplots for all categorical variables in the dataset.\n",
    "\n",
    "        Optionally groups the bars using a hue column (e.g., 'churn_target'), allowing \n",
    "        visual comparison of class distributions between different categories. Annotates\n",
    "        each bar with its percentage frequency.\n",
    "\n",
    "        Args:\n",
    "            hue (str, optional): Name of the column used to group bars (e.g., target variable).\n",
    "                                If None, no grouping is applied.\n",
    "            num_columns (int): Number of plots per row in the subplot grid.\n",
    "            figsize_per_row (int): Height (in inches) of each subplot row.\n",
    "            palette (list): List of colors to use when `hue` is specified.\n",
    "            color (str): Default color to use when `hue` is not provided.\n",
    "            title (str): General title for the entire plot grid.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the hue column is not found in the DataFrame.\n",
    "            Exception: If the plot generation fails for unexpected reasons.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if hue and hue not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{hue}' not found in the DataFrame.\")\n",
    "\n",
    "            categorical_cols = self.data.select_dtypes(include = ['object', 'category', 'int8']).columns.tolist()\n",
    "            if hue and hue in categorical_cols :\n",
    "                categorical_cols.remove(hue)\n",
    "            \n",
    "            # Config Ax's\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row, h_size = 30)\n",
    "\n",
    "            for i, column in enumerate(categorical_cols):\n",
    "                sns.countplot(\n",
    "                    data = self.data,\n",
    "                    x = column,\n",
    "                    hue = hue if hue else None,\n",
    "                    palette = palette if hue else None,\n",
    "                    color = None if hue else color,\n",
    "                    edgecolor = 'white' if hue else 'black',\n",
    "                    saturation = 1,\n",
    "                    alpha = 0.8,\n",
    "                    legend = False,\n",
    "                    ax = ax[i]\n",
    "                )\n",
    "                \n",
    "                total = len(self.data[column])\n",
    "                for p in ax[i].patches:\n",
    "                    height = p.get_height()\n",
    "                    if height == 0:\n",
    "                        continue\n",
    "                    percentage = f'{100 * height / total:.1f}%'\n",
    "                    x = p.get_x() + p.get_width() / 1.95\n",
    "                    y = height\n",
    "                    ax[i].annotate(\n",
    "                        percentage,\n",
    "                        (x, y),\n",
    "                        ha = 'center',\n",
    "                        va = 'bottom',\n",
    "                        fontsize = 16,\n",
    "                        fontweight = 'bold',\n",
    "                        color = 'black'\n",
    "                    )\n",
    "\n",
    "                # Config Ax's\n",
    "                self._format_single_ax(ax[i], f'Variable: {column}')\n",
    "                ax[i].set_xticks(range(len(ax[i].get_xticklabels())))\n",
    "                ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize = 16)\n",
    "                \n",
    "            # Show Graphics\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to generate categorical countplots: {str(e)}')\n",
    "\n",
    "\n",
    "    # ======================================================== #\n",
    "    # Categorical Bar Percentages - Function                   #\n",
    "    # ======================================================== #\n",
    "    def categorical_bar_percentages(\n",
    "        self,\n",
    "        hue: str ,\n",
    "        palette: list = ['#1abc9c', '#ff6b6b'],\n",
    "        num_columns: int = 3,\n",
    "        figsize_per_row: int = 8,\n",
    "        title: str = 'Barplots Of The Individual Rate Percentages Of Each Column Class'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots barplots of churn percentages per class of each categorical variable.\n",
    "\n",
    "        This method calculates the percentage distribution of a binary target (`hue`)\n",
    "        within each category of all categorical columns in the dataset, and visualizes\n",
    "        these percentages as barplots.\n",
    "\n",
    "        Args:\n",
    "            hue (str): Name of the binary target column (e.g., 'churn_target').\n",
    "            palette (list, optional): List of colors for the hue classes.\n",
    "                Defaults to ['#b0ff9d', '#db5856'].\n",
    "            num_columns (int): Number of subplots per row in the grid.\n",
    "            figsize_per_row (int): Height (in inches) allocated per subplot row.\n",
    "            title (str): Overall title for the figure.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `hue` is not found in the DataFrame.\n",
    "            Exception: For other errors during computation or plotting.\n",
    "\n",
    "        Returns:\n",
    "            None: Displays the plot directly.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if hue and hue not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{hue}' not found in the DataFrame.\")\n",
    "            categorical_cols = self.data.select_dtypes(include = ['object', 'category', 'int8']).columns.tolist()\n",
    "            if hue and hue in categorical_cols:\n",
    "                categorical_cols.remove(hue)\n",
    "\n",
    "            # Define AX and Fig\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row, h_size = 30)\n",
    "\n",
    "            for i, column in enumerate(categorical_cols):\n",
    "                \n",
    "                total_churn_per_class = self.data.groupby(column, observed = True)[hue].count().reset_index(name = f'total_count_class')\n",
    "\n",
    "                result = (\n",
    "                    self.data.groupby([column, hue], observed = True)[hue]\n",
    "                    .count()\n",
    "                    .reset_index(name = 'frequency')\n",
    "                    .merge(total_churn_per_class, on = column)\n",
    "                )\n",
    "                result['percentage_per_class'] = round((result['frequency'] / result['total_count_class']) * 100, 2)\n",
    "\n",
    "                sns.barplot(\n",
    "                    data=result,\n",
    "                    x = column,\n",
    "                    y = 'percentage_per_class',\n",
    "                    hue = hue,\n",
    "                    palette = palette,\n",
    "                    edgecolor = 'white',\n",
    "                    saturation = 1,\n",
    "                    legend = False,\n",
    "                    ax = ax[i]\n",
    "                )\n",
    "\n",
    "                # Annotate bars\n",
    "                for p in ax[i].patches:\n",
    "                    height = p.get_height()\n",
    "                    percentage = f'{height:.1f}%'\n",
    "                    x = p.get_x() + p.get_width() / 2\n",
    "                    ax[i].annotate(\n",
    "                        percentage,\n",
    "                        (x, height),\n",
    "                        ha='center',\n",
    "                        va='bottom',\n",
    "                        fontsize=14,\n",
    "                        fontweight = 'bold',\n",
    "                        color='black'\n",
    "                    )\n",
    "\n",
    "                # Config Ax's\n",
    "                self._format_single_ax(ax[i], f'Variable: {column}')\n",
    "                ax[i].set_xticks(range(len(ax[i].get_xticklabels())))\n",
    "                ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize = 16)\n",
    "            \n",
    "            # Show Graphics\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to generate percentage barplots: {str(e)}.')\n",
    "\n",
    "    # ======================================================== #\n",
    "    # Target Analysis - Function                               #\n",
    "    # ======================================================== #\n",
    "    def plot_target_analysis(\n",
    "        self, \n",
    "        target_col: str,\n",
    "        title: str = 'Target Variable Distribution',\n",
    "        colors: list =['#1abc9c', '#ff6b6b'],\n",
    "        palette: str = 'RdYlBu_r',\n",
    "        figsize: tuple = (14, 6)\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Generates a panel for in-depth analysis of the target variable.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if target_col not in self.data.columns:\n",
    "                raise ValueError(f\"Target '{target_col}' not found in Dataframe.\")\n",
    "\n",
    "            # Prepraration of Data (Ordered by Frequency: Highest -> Lowest)\n",
    "            counts = self.data[target_col].value_counts()\n",
    "            labels = counts.index\n",
    "            values = counts.values\n",
    "\n",
    "            # Color Logic\n",
    "            if len(labels) <=2:\n",
    "                final_colors = colors\n",
    "            else:\n",
    "                final_colors = sns.color_palette(palette, n_colors = len(labels))\n",
    "\n",
    "            # Define AX and Fig\n",
    "            fig, ax = plt.subplots(1, 2, figsize = figsize)\n",
    "            plt.suptitle(title, fontsize = 20, fontweight = 'bold', y = 1.02)\n",
    "\n",
    "            # Plot 1: Donut Chart\n",
    "            wedges, texts, autotexts = ax[0].pie(\n",
    "                values, \n",
    "                labels = labels,\n",
    "                autopct = '%1.1f%%',\n",
    "                startangle = 90,\n",
    "                colors = final_colors,\n",
    "                pctdistance = 0.80,\n",
    "                wedgeprops =  dict(width = 0.4, edgecolor = 'white'),\n",
    "                textprops = {'fontsize': 14}\n",
    "            )\n",
    "\n",
    "            ax[0].set_title(f'Class Balance (%)', fontsize = 16, fontweight = 'bold')\n",
    "            for autotext in autotexts:\n",
    "                autotext.set_color('black')\n",
    "                autotext.set_weight('bold')\n",
    "\n",
    "            # Plot 2: Bar Plot\n",
    "            sns.barplot(\n",
    "                x = labels,\n",
    "                y = values,\n",
    "                hue = labels,\n",
    "                palette = final_colors,\n",
    "                order = labels,\n",
    "                hue_order = labels,\n",
    "                legend = False,\n",
    "                ax = ax[1],\n",
    "                edgecolor = 'black'\n",
    "            )\n",
    "\n",
    "            ax[1].set_title(f'Absolute Frequecy (N)', fontsize = 16, fontweight = 'bold')\n",
    "            ax[1].set_ylabel('Count') \n",
    "            ax[1].grid(axis = 'y', linestyle = '--', alpha = 0.5)\n",
    "            ax[1].grid(axis = 'x', linestyle = '--', alpha = 0.5)\n",
    "\n",
    "            # Annotate of values for blarplot\n",
    "            for p in ax[1].patches:\n",
    "                height = p.get_height()\n",
    "                if height > 0:\n",
    "                    ax[1].annotate(\n",
    "                    f'{int(height)}',\n",
    "                    (p.get_x() + p.get_width() / 2., height),\n",
    "                    ha = 'center', \n",
    "                    va = 'bottom',\n",
    "                    fontsize = 14,\n",
    "                    fontweight = 'bold',\n",
    "                    color = 'black'\n",
    "                    )\n",
    "            \n",
    "            plt.tight_layout(rect = [0, 0, 1, 0.97])\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to plot target analysis: {str(e)}')\n",
    "\n",
    "    # ======================================================== #\n",
    "    # Correlation Heatmap - Function                           #\n",
    "    # ======================================================== #\n",
    "    def correlation_heatmap(\n",
    "        self,\n",
    "        title: str = None,\n",
    "        cmap: str = 'RdBu_r',\n",
    "        h_size: int = 20,\n",
    "        v_size: int = 15\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots a heatmap showing the correlation matrix among the numerical columns.\n",
    "\n",
    "        This method computes the correlation matrix of the dataset and displays it as a heatmap,\n",
    "        with annotations showing the correlation coefficients.\n",
    "\n",
    "        Args:\n",
    "            title (str, optional): Title for the heatmap plot. Defaults to None.\n",
    "            cmap (str, optional): Colormap to use for the heatmap. Defaults to 'coolwarm'.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the heatmap generation or plotting fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Select only the desired columns\n",
    "            corr_data = self.data.corr()\n",
    "            sns.set_style('white')\n",
    "            # Define AX and Fig\n",
    "            plt.rc('font', size = 15)\n",
    "            fig, ax = plt.subplots(figsize = (h_size, v_size))\n",
    "            \n",
    "            sns.heatmap(\n",
    "                corr_data,\n",
    "                mask = np.triu(np.ones_like(corr_data, dtype = bool)),\n",
    "                annot = True,\n",
    "                cmap = cmap,\n",
    "                center =  0,\n",
    "                vmax = 1,\n",
    "                vmin = -1,\n",
    "                fmt = '.2f',\n",
    "                linewidths = .5,\n",
    "                cbar_kws = {'shrink': .8},\n",
    "                ax = ax\n",
    "            )\n",
    "            # Config Ax's and Show Graphics\n",
    "            ax.set_title(title, fontsize = 20, fontweight = 'bold')\n",
    "            plt.tight_layout(rect = [0, 0, 1, 0.97])\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f'[Error] Failed to generate correlation heatmap: {str(e)}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47872334-51bc-4c5f-8a01-89b60093cc53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae624b9-31dc-49c2-9031-1e522bf51f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ChurnData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197d306c-26b3-4f50-89c6-eae69daba922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verify successful load with some randomly selected records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b87af9-26b9-4310-9a69-0e44024f61c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.sample(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c85e3dc-f9b0-4090-8be0-63217e9e6231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Business Understanding: \n",
    "\n",
    "#### 1.1 Situation Assessment\n",
    "The Telecommunications industry faces saturation and fierce competition. With high Customer Acquisition Costs (CAC), the profitability strategy has shifted from **Acquisition** to **Retention**. \n",
    "Current analysis shows that losing a long-standing customer (\"High Tenure\") is 5x more expensive than acquiring a new one due to the loss of predictable recurring revenue (LTV).\n",
    "\n",
    "#### 1.2 The Business Problem\n",
    "The company is experiencing an unexplained increase in customer attrition (Churn). Traditional rule-based methods (e.g., \"Cancel if usage drops 10%\") are reactive and fail to capture complex behavioral patterns. \n",
    "The marketing team needs a proactive mechanism to identify at-risk customers **before** the cancellation decision is irreversible.\n",
    "\n",
    "#### 1.3 Objectives\n",
    "1.  **Primary Goal:** Mitigate revenue loss by accurately identifying customers with high probability of churning.\n",
    "2.  **Strategic Goal:** Understand the *drivers* of churn (Explainability). Is it price (`wiremon`) or poor service? This informs the product roadmap.\n",
    "3.  **Financial Goal:** Maximize the **ROI of Retention**.\n",
    "    * *Constraint:* We cannot offer discounts to everyone (Cost of Intervention). We must target only high-risk/high-value customers.\n",
    "\n",
    "#### 1.4 Success Criteria (KPIs)\n",
    "Instead of purely technical metrics (Accuracy), success is defined by:\n",
    "* **Lift in Top Decile:** The model must capture at least 30% of total churners within the top 10% risk list.\n",
    "* **Precision (Cost of False Positives):** Minimizing \"False Alarms\" to avoid giving discounts to customers who would have stayed anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c607f560-21ac-4529-a31a-37e738d44042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data Understanding:\n",
    "---\n",
    "\n",
    "#### Dataset: `Telecommunications Churn Database`\n",
    "\n",
    "- This dataset captures a historical snapshot of customer demographics, service usage patterns, and account status. The primary objective is to correlate these features with the attrition event (**Churn**) to isolate behavioral drivers of dissatisfaction.\n",
    "---\n",
    "\n",
    "#### Variables Dictionary:\n",
    "\n",
    "**1. Target Variable (The Outcome)**\n",
    "* **churn** *Integer (Binary)* - The classification target. `1` = Customer left (Voluntary Churn); `0` = Stayed.\n",
    "\n",
    "**2. Customer Demographics & Profile**\n",
    "* **custcat** *Categorical* - Customer category classification (1-4). Represents the customer segment/cluster.\n",
    "* **tenure** *Integer* - Months with the company (Proxy for *Loyalty*).\n",
    "* **age** *Integer* - Customer's age in years.\n",
    "* **address** *Integer* - Years living at current address (Stability indicator).\n",
    "* **ed** *Categorical* - Education level (1-5).\n",
    "* **employ** *Integer* - Years with current employer.\n",
    "* **income** *Continuous* - Annual household income (in thousands).\n",
    "\n",
    "**3. Service Subscriptions (Binary Portfolio)**\n",
    "* **ebill** *Binary* - Electronic billing subscription (0/1). (Digital Adoption indicator).\n",
    "* **equip** *Binary* - Equipment rental (0/1).\n",
    "* **callcard** *Binary* - Calling card service (0/1).\n",
    "* **wireless** *Binary* - Wireless service (0/1).\n",
    "* **pager** *Binary* - Pager service (Legacy technology).\n",
    "* **internet** *Binary* - Internet service (0/1).\n",
    "* **voice** *Binary* - Voice mail service (0/1).\n",
    "* **callwait** *Binary* - Call waiting service (0/1).\n",
    "* **confer** *Binary* - Conference calling service (0/1).\n",
    "\n",
    "**4. Billing & Usage (Monthly Dynamics)**\n",
    "* **longmon** *Continuous* - Average monthly long-distance usage ($).\n",
    "* **tollmon** *Continuous* - Average monthly toll-free usage ($).\n",
    "* **equipmon** *Continuous* - Average monthly equipment rental charges ($).\n",
    "* **cardmon** *Continuous* - Average monthly calling card charges ($).\n",
    "* **wiremon** *Continuous* - Average monthly wireless service charges ($).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8acfb1a5-83a6-4d4b-adcd-4e6c4013a344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Exploratory Data Analysis (EDA):\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516b09f2-2f65-4524-a880-b0c8bef27040",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Univariate Analysis:\n",
    "---\n",
    "\n",
    "Examines the behavior of **a single variable** in isolation to understand its distribution, central tendency, and dispersion (e.g., histograms and means), without seeking relationships with other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56ef6fc-9d60-45a3-aadf-473c9ef4902d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cfb6420-fef4-43a3-ac20-9352c4b90c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99781e41-3c66-402a-ae89-2547934fdc0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Adjusting the variable types with their respective characteristics. \n",
    "---\n",
    "- In this data, there are both binary and ordinal variables; I will be adjusting them so that there is no invalid statistical aggregation in the analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "453c947b-3378-4ba1-864b-f2d77dc49de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def optimize_dtypes(df):\n",
    "    \"\"\"\n",
    "    Adjusting the variable types with their respective characteristics. In this data, there are both binary and ordinal variables; I will be adjusting them so that there is no invalid statistical aggregation in the analyses.\n",
    "    \"\"\"\n",
    "    # Copy DF\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Categorical Columns\n",
    "    categorical_cols = ['custcat', 'ed',]\n",
    "\n",
    "    # Interger Columns\n",
    "    integer_cols =  ['tenure', 'age', 'address', 'employ']\n",
    "\n",
    "    # Binary Columns\n",
    "    binary_cols = [\n",
    "        'equip','callcard', 'wireless','ebill', 'voice', 'pager',\n",
    "        'internet', 'callwait', 'confer','churn'\n",
    "    ]\n",
    "\n",
    "    # Continus Columns\n",
    "    continuos_cols = [\n",
    "        'income',  'longmon', 'tollmon', 'equipmon', 'cardmon',\n",
    "        'wiremon',\n",
    "    ]\n",
    "\n",
    "    # Categorical Columns\n",
    "    df_clean[categorical_cols] = df_clean[categorical_cols].astype('int32')\n",
    "    df_clean[categorical_cols] = df_clean[categorical_cols].astype('category')\n",
    "\n",
    "    # Interger Columns\n",
    "    df_clean[integer_cols] = df_clean[integer_cols].astype('Int64')\n",
    "\n",
    "    # Binary Columns\n",
    "    df_clean[binary_cols] = df_clean[binary_cols].astype('int8')\n",
    "\n",
    "    # Continus Columns\n",
    "    df_clean[continuos_cols] = df_clean[continuos_cols].astype('float32')\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "df = optimize_dtypes(df)\n",
    "\n",
    "print(f'New dtypes of variables:')\n",
    "df.info()\n",
    "\n",
    "print(f'Visual sample:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c019ee-6d71-4925-80c5-e390995b4ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f04ea8-c533-4e03-a5fa-7968e5a14f02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[df.duplicated(keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d5fe4ab-03dc-447f-bef3-0a9ae011ca8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_describe = df.describe()\n",
    "data_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b16bf4b-63ab-42e5-9128-cfa87bf03f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_describe.loc['min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac705fad-61a1-4c4f-95ca-fa4547d71fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_describe.loc['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb113c0-4d8d-49de-a343-c792bf37a015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_describe.loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd44b25-dee4-4732-97b3-9293d76b69d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numericals_cols = [\n",
    "    'tenure', 'age', 'employ', 'address', 'income', 'longmon', 'tollmon', 'equipmon', 'cardmon', 'wiremon',\n",
    "]\n",
    "GraphicsData(data = df[numericals_cols]).numerical_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b00b35e-f644-4cbd-9bbf-521e21792eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "GraphicsData(data = df[numericals_cols]).numerical_boxplots(showfliers = True, showmeans = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c43f0a7e-435b-422f-b7ce-ce5560213e18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'ed', 'equip', 'callcard', 'wireless', 'voice', 'pager', 'internet',\n",
    "    'callwait', 'confer', 'ebill', 'custcat'\n",
    "]\n",
    "GraphicsData(data = df[categorical_cols]).categorical_countplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b5ce6e-c406-4f1a-b70d-b2fd053af03b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GraphicsData(data = df).plot_target_analysis(target_col='churn', colors=['#1abc9c', '#ff6b6b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072b9b94-5a80-47fc-a40c-9c83741a2cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Key Observations:\n",
    "---\n",
    "**1. Data Integrity and Quality**\n",
    "- The dataset exhibits high integrity, containing 200 records and 22 active variables. The absence of null or duplicate data eliminates the need for imputation, allowing immediate focus on statistical modeling.\n",
    "---\n",
    "**2. Distribution of Numerical Variables**\n",
    "\n",
    "  - **Income:** There is extreme skewness (**9.96**). The scenario reflects a concentration of income: a vast majority with standard income and an elite of \"Super Rich\" (representing the **6.5% of outliers**). **Spending Pattern (`longmon`, `cardmon`, `wiremon`):** Consumption variables follow the income curve. The strong skewness confirms that spending behavior is directly proportional to purchasing power.\n",
    "\n",
    "  - **Niche Services (`tollmon`, `equipmon`):** These distributions are \"Zero-Inflated,\" indicating that a significant portion of the base does not even use these specific services.\n",
    "\n",
    "  - **Demographics (`age`):** The base is balanced, with a predominance of customers up to 50 years old, but with a tail extending to 76 years old, suggesting demographic maturity.\n",
    "\n",
    "  - **Loyalty (`tenure`):** The bimodal/flat distribution indicates a healthy life cycle: the company manages to attract new customers (entrants) while retaining a loyal older base.\n",
    "---\n",
    "**3. Outlier Analysis**\n",
    "  - The volume of outliers is not critical to the size of the dataset. The discrepancies in `income` and `longmon` are not errors, but natural characteristics of high-value customers (Whales). *Strategy: Treat with mathematical transformations instead of removal.*\n",
    "---\n",
    "**4. Portfolio Analysis (Categorical Variables)**\n",
    "\n",
    "  - **Educational Profile (`ed`):** Concentration in middle levels (2 and 4). Level 5 (Postgraduate) is a minority, deserving specific investigation regarding its retention.\n",
    "\n",
    "  - **Low Adoption:** Products such as `wireless`, `voice`, and `pager` have penetration below **30%**. They are candidates for *Upsell* campaigns or product review.\n",
    "\n",
    "  - **Flagship Product:** The `callcard` dominates with **70.05%** adoption, proving to be the company's essential entry-level product.\n",
    "\n",
    "  - **Moderate Adoption:** `internet`, `ebill`, and `confer` are in the intermediate range. They are not niche products, but have great growth potential in the current base.\n",
    "---\n",
    "**5. Churn Rate (29%)** \n",
    "  - The cancellation rate of **29%** is at the upper limit of the telecommunications market average. Although the sector is volatile, losing almost 1/3 of the customer base annually requires immediate, data-driven retention actions.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4015c0-3d81-4858-83d2-d14bdb11077c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Bi-Variate Analysis:\n",
    "---\n",
    "\n",
    "Explores the mathematical relationship between **two variables** simultaneously to discover associations, correlations, or dependencies (e.g., scatterplot of Income vs. Debt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "405ca2dc-c235-491b-b533-0bea5ca9cfad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Note: \n",
    "---\n",
    "- For the execution of the bivariate analysis, I will perform a prior division of the data into **Train** and **Test** sets. The primary objective is to avoid **Data Leakage**, ensuring that all insights, outlier treatments, and feature engineering are derived exclusively from the training set.\n",
    "\n",
    "- In addition, I will use the `stratify` parameter in the `train_test_split` function. Since this is a classification problem (churn prediction), it is mandatory to maintain the same **proportion of classes** of the target variable in both subsets, preserving the original statistical representativeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156c5833-e114-4e2a-82e8-eb6934922518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df, test_size = 0.2, stratify = df['churn'], shuffle = True, random_state = 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c4c3b45-b760-4649-8ac4-13f9a6327e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking the proportions of the target variable\n",
    "print(f'Shape of training: {train_set.shape}')\n",
    "print(f'Shape of test: {test_set.shape}')\n",
    "\n",
    "print('\\n--- Churn Rate (Stratify Validation) ---')\n",
    "print(f'Original: {df['churn'].mean():.2%}')\n",
    "print(f'Train:    {train_set['churn'].mean():.2%}')\n",
    "print(f'Test:    {test_set['churn'].mean():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e499801-08a1-470a-baaf-46d20f7f98a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3028ce31-f4bd-4f92-867c-456474abd179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking the correlations between the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b48e32-b739-48f2-9c07-0a7df276ab91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_set.corr()['churn'].abs().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac04fb8-cfb0-4d8f-a458-46ac55f55248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GraphicsData(data = train_set).correlation_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4cd30a1-5617-4678-904b-86d58aca6fb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Key Observations:\n",
    "---\n",
    "**1. Retention Factors (Negative Correlation)**\n",
    "\n",
    "   - **Tenure (`tenure`):** Shows the strongest negative correlation (**-0.35**) with Churn. This confirms the premise of \"Survival Analysis\": the probability of cancellation decreases drastically as customer tenure increases. The critical customer is the newcomer.\n",
    "\n",
    "   - **Stability (`address`, `employ`, `age`):** Variables that denote life stability (time at address, stable employment, and advanced age) are strong protectors against Churn. The profile of a loyal customer is mature and conservative.\n",
    "\n",
    "   - **Voice Usage (`longmon`, `callcard`):** Customers with high consumption of voice services (long distance and calling cards) tend to be more loyal. This suggests that the \"Voice\" product generates greater *lock-in* than digital products.\n",
    "---\n",
    "**2. Risk Factors (Positive Correlation)**\n",
    "\n",
    "   - **Education (`ed`):** With a positive correlation (**0.24**), it is observed that customers with higher levels of education tend to cancel more. This suggests that more educated consumers are more demanding, research the competition more, and have a lower informational barrier to switching operators.\n",
    "\n",
    "   - **Digital Services (`equip`, `internet`, `wireless`):** Paradoxically, the contracting of modern services is associated with **higher Churn**.\n",
    "\n",
    "   - *Hypothesis:* These services operate in highly competitive markets (commodities), where the customer switches providers for small price differences, unlike traditional voice service.\n",
    "\n",
    "   - **Digital Billing (`ebill`):** The positive correlation with Churn (**0.21**) suggests a behavioral profile. Users of `ebill` tend to be younger and more digitally savvy, possessing a lower switching cost and lower brand loyalty than users of physical invoices.\n",
    "---\n",
    "**3. Neutral and Categorical Variables**\n",
    "\n",
    "- **Income:** The low linear correlation (-0.09) reflects the high asymmetry (Skewness 9.96). The relationship between money and churn is probably not linear, requiring transformations (Log) to reveal its true predictive power.\n",
    "\n",
    "- **Category (`custcat`**): Being nominal, its influence will be validated via visual analysis, since Pearson does not capture nuances of non-ordinal categories.\n",
    "---\n",
    "**4. Multicollinearity Alert**\n",
    "There are some critical redundancies between service ownership and monthly cost indicators:\n",
    "   - **`equip` vs `equipmon`** (0.95)\n",
    "   - **`wireless` vs `wiremon`** (0.89)\n",
    "   - *For linear models (Logistic Regression), it will be mandatory to remove the continuous cost variables (`...mon`) in favor of the binary ones, or vice versa, to avoid instability in the coefficients.*\n",
    "\n",
    "   - **`longmon` vs `ternure`** (0.77): This correlation means that older customers spend more on long-distance calling services. There is a possibility that the model will become unstable if these 2 variables are maintained, because the correlation between them, although not perfect, is significant.\n",
    "\n",
    "   - **`address` vs `age`** (0.74): There is a hypothesis that they are passing similar information but not the same information. Older customers tend to stay longer at the same address, as this behavior relates to *Stability*.\n",
    "   Contextually, it makes sense for one variable to have a linear relationship with the other. However, in this case, it is worth evaluating these two variables and their possible influence on the target variable `churn`.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3818dff6-3de3-45a6-a1c4-91e5349e2b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Analyzing the numerical variables and their relationships with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ddbde9-d9b6-4e60-b046-3930a15265ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numericals_cols = [\n",
    "    'tenure', 'age', 'employ', 'address', 'income', 'longmon', 'tollmon', 'equipmon', 'cardmon', 'wiremon', 'churn'\n",
    "]\n",
    "GraphicsData(data = df[numericals_cols]).numerical_histograms(hue = 'churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d3c75e-de10-49e7-b647-51299388610c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GraphicsData(data = df[numericals_cols]).numerical_boxplots(hue = 'churn', showmeans = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b18e39b-d983-4e4f-800f-fce33b3808f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Key Observations:\n",
    "---\n",
    "**1. Customer Lifecycle (`tenure`)**\n",
    "\n",
    "  - **Averages:** Churners (22.78 months) vs. Non-Churners (40.22 months).\n",
    "\n",
    "  - **Survival Analysis:** There is a \"Critical Risk Zone\" between 0 and 22 months. The average churn rate at 22 months indicates that if the institution manages to retain the customer beyond this initial 2-year barrier, the probability of loyalty doubles.\n",
    "\n",
    "  - **Retention Insight:** Customers who exceed the 50-month mark become practically immune to churn (\"total lock-in\"). The retention strategy should be aggressive in the first year (onboarding) to push the customer base into the safe zone of 40+ months.\n",
    "---\n",
    "**2. The Stability Factor (`age`, `employ`, `address`)**\n",
    "\n",
    "  - **The Pattern:** There is a direct correlation between life stability and loyalty.\n",
    "\n",
    "  - **Age:** Loyal customers are, on average, 8 years older (43 years) than churners (35 years). The age group over 65 years old shows almost zero churn.\n",
    "\n",
    "  - **Employment:** The employment time of retained customers (12 years) is more than **double** that of those who cancel (5.5 years).\n",
    "\n",
    "  - **Residence:** Address stability follows the same proportion (13 years vs. 7.5 years).\n",
    "\n",
    "  - **Diagnosis:** Churn at this institution is not just a matter of dissatisfaction with the service, but a reflection of the **financial volatility** of the younger and more unstable demographic profile. Cancellation can often be involuntary (non-payment) or due to price sensitivity.\n",
    "---\n",
    "**3. The Paradox of Income and Digital Services (`income`, `wiremon`, `equipmon`)**\n",
    "\n",
    "  - **Economic Scenario:** Customers who cancel have an average income 31% lower (56k) than loyal customers (82k).\n",
    "\n",
    "  - **The Paradox:** Despite having lower income and less stability, churners are the ones who most often contract (and spend on) \"premium\" and modern services, such as wireless and equipment rental (`equipmon`).\n",
    "\n",
    "  - **Business Conclusion:** There is a **Product-Market Fit** misalignment. Expensive products (Wireless/Equipment) are being sold to an audience with lower purchasing power and high instability (young people), generating default or rapid cancellation. Meanwhile, the loyal audience (wealthy and mature) consumes cheap and legacy products.\n",
    "---\n",
    "**4. Loyalty Anchor (`longmon`, `cardmon`)**\n",
    "\n",
    "  - **Voice Products:** Unlike digital services, voice usage (long distance) is the major differentiator for loyalty. Retained customers spend almost twice as much (13.6 vs. 7.2) on long-distance calls.\n",
    "\n",
    "  - **Insight:** The voice service acts as an \"anchoring\" product. Those who use the phone to talk, stay. Those who use it for internet/data, leave.\n",
    "---\n",
    "**5. Noise Variables (`tollmon`)**\n",
    "\n",
    "- **Irrelevance:** The `tollmon` variable (toll/extra charges) does not present a clear statistical distinction between the means or distributions of the two groups. It is suggested to evaluate its removal (Feature Selection) to reduce the complexity of the model without loss of information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b58e6210-eeba-4be0-8a0f-ca5369aa2a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Analyzing the categorical variables and their relationships with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54ff2aa-19fb-4945-b737-652b5b052a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'ed', 'equip', 'callcard', 'wireless', 'voice', 'pager', 'internet',\n",
    "    'callwait', 'confer', 'ebill', 'custcat', 'churn'\n",
    "]\n",
    "GraphicsData(data = df[categorical_cols]).categorical_countplots(hue = 'churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d98d07f-cbde-4da2-92b7-9f7558edfb7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GraphicsData(data = df[categorical_cols]).categorical_bar_percentages(hue = 'churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e9b19dd-2f6c-49ea-bdd7-666b23a6dd57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Key Observations\n",
    "\n",
    "---\n",
    "\n",
    "**1. Educational Level (`ed`)**\n",
    "\n",
    "* **The Effect of \"Smart Switching\":** A linear increase in the cancellation rate is observed as the educational level increases, reaching a critical peak of **47.1%** at Level 5.\n",
    "* **Diagnosis:** Customers with a high level of education treat communications as a *commodity*. They have **low switching costs** and make rational decisions. Unlike the inert base, this profile compares market prices. The prevention strategy here should be based on financial advantage, not emotional.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Digital Services and Equipment (`equip`, `internet`, `wireless`)**\n",
    "\n",
    "* **The Toxic Trio:** Data reveals a structural flaw in these value-added services. Retention drops significantly when customers acquire them.\n",
    "\n",
    "* **Internet:** The cancellation rate jumps from **18.8%** to **42.0%**.\n",
    "\n",
    "* **Equipment:** Even more critical, churn explodes from **18.3%** to **43.5%**.\n",
    "\n",
    "* **Product-Market Fit Alert:** This indicates that the company's digital infrastructure and hardware rental policies are acting as **churn accelerators**. Selling these products to at-risk segments (such as young users) is effective in pushing the competition.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Voice Services (phone card, conferencing, voice)**\n",
    "\n",
    "* **Retention Anchors (with an imposter):** Functional voice features generally generate high loyalty (retention), but there is a clear divide.\n",
    "\n",
    "* **Phone card:** A massive retention factor. Users canceled significantly less (**19.9%**) than non-users (**50.9%**).\n",
    "\n",
    "* **Conference and Call Waiting:** Both confirm the rule, showing lower churn rates for users.\n",
    "\n",
    "* **The \"Toxic\" Intruder (`voice`):** Voicemail defies the category trend. Users canceled the service significantly more (**39.0%**) than non-users (**24.8%**). This suggests that a `voice` behaves more like a deficient \"Digital Service\" (generating friction/cost) than a useful \"Voice Service\".\n",
    "\n",
    "---\n",
    "\n",
    "**4. Customer Segmentation (`customer`)**\n",
    "\n",
    "* **Scalability Alert:** The disparity between segments is extreme.\n",
    "\n",
    "* **Class 3:** The \"Gold Segment\" with only **8.3%** cancellation rate.\n",
    "\n",
    "* **Class 4:** The \"Danger Zone\" with a **45.6%** cancellation rate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc78ce3-a904-4295-b433-ae8cb61aeed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Multi-Variate Analysis:\n",
    "---\n",
    "\n",
    "Analyzes a set of **three or more variables** at the same time to understand complex interactions and latent structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd1072e-8ac7-4f0c-a8ca-d6c848ba044d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Optimization of Numerical Variables\n",
    "\n",
    "- Some financial and consumption variables have \"long tails\" (many clients with low values, few with extreme values), which distorts the predictive capacity of some algorithms.\n",
    "\n",
    "- **Action:** A mathematical (Logarithmic) normalization will be applied to balance these distributions. Objective: To \"unlock\" hidden patterns of behavior, allowing the model to better differentiate at-risk clients, regardless of their income or consumption bracket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95cbcd89-8a46-4c5a-a95e-2a250429bd16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Columns for transfomations\n",
    "skew_columns = ['income', 'longmon', 'cardmon', 'wiremon']\n",
    "\n",
    "for col in skew_columns:\n",
    "    train_set[f'log_{col}'] = np.log1p(train_set[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7bd5d41-a287-4ec3-acc0-06e2bb84773e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3699f2b-734a-4489-a4d5-4c7f3c91a42a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skew_columns = ['income', 'longmon', 'cardmon', 'wiremon', 'log_income', 'log_longmon', 'log_cardmon', 'log_wiremon', 'churn']\n",
    "GraphicsData(data = train_set[skew_columns]).numerical_histograms(hue = 'churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93d29321-8695-4158-9a47-e556b817e44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skew_columns = ['income', 'longmon', 'cardmon', 'wiremon', 'log_income', 'log_longmon', 'log_cardmon', 'log_wiremon', 'churn']\n",
    "GraphicsData(data = train_set[skew_columns]).correlation_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53be437b-07d3-4179-8caf-1ba35b79265f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Note:\n",
    "---\n",
    "**Impact Analysis: Log-Transformation**\n",
    "\n",
    "- **Mitigation of Skewness:** The application of the `log1p` transformation was effective in normalizing the distributions, drastically reducing the skewness of the critical variables.\n",
    "\n",
    "- **Sign Gain (Pearson Correlation):** There was a tangible increase in linearity with the target variable (`churn`):\n",
    "- **Income:** Increase from **0.09** to **0.13** (+44% relative gain).\n",
    "\n",
    "- **Longmon:** Refinement from **0.26** to **0.29**.\n",
    "\n",
    "- **Cardmon (Highlight):** The most significant jump, doubling its relevance from **0.13** to **0.24**.\n",
    "\n",
    "- **Discovery of Latent Patterns:** The transformation in `cardmon` revealed a hidden **bimodal** structure in the raw data. The logarithm visually separated two distinct subgroups of behavior:\n",
    "\n",
    "  - 1. **Peak on the Left:** Casual Users (*Low usage*).\n",
    "\n",
    "  - 2. **Peak on the Right:** Heavy Users (*Heavy users*).\n",
    "\n",
    "*This will facilitate the creation of decision cuts by tree-based models.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c2c0e9-093d-49cd-b20d-6fd81d070ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Creating News Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f41c7f-c139-4ffb-8e95-8b3560ac8f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregations of costs (Total Wallet Share)\n",
    "# All expenses related to monthly services will be added together.\n",
    "mon_cols = ['longmon', 'tollmon', 'equipmon', 'cardmon','wiremon']\n",
    "train_set['total_spend'] = train_set[mon_cols].sum(axis = 1)\n",
    "\n",
    "# Accessibility Index (Affordability)\n",
    "# Since 'income' is in thousands (e.g., 20 = 20,000), I adjusted the scale.\n",
    "# I added +1 to the denominator to avoid division by zero (safety).\n",
    "train_set['affordability_idx'] = train_set['total_spend'] / ((train_set['income'] * 1000) + 1)\n",
    "\n",
    "# Risk Feature (Toxicity + Education)\n",
    "toxic_list = ['internet', 'wireless', 'equip', 'voice', 'pager']\n",
    "train_set['toxic_score'] = train_set[toxic_list].sum(axis = 1)\n",
    "train_set['toxic_ed'] = (train_set['toxic_score'] * train_set['ed'].astype('int64')).astype('float32')\n",
    "\n",
    "# Behavioral Usage Features\n",
    "# Ternure for longom\n",
    "train_set['ternure_longmon'] = ((train_set['tenure'] / 12) * (train_set['longmon']) ).astype('float32')\n",
    "# Ternure for cardmon\n",
    "train_set['ternure_cardmon'] = ((train_set['tenure'] / 12) * (train_set['cardmon']) ).astype('float32')\n",
    "\n",
    "# Stability Features\n",
    "# Ternure for age \n",
    "train_set['stability_age'] =  ((train_set['tenure'] / 12) * (train_set['age'] - 18) ).astype('float32')\n",
    "# Ternure for address \n",
    "train_set['stability_address'] = ((train_set['tenure'] / 12) * (train_set['address']) ).astype('float32')\n",
    "# Ternure for address \n",
    "train_set['stability_employ'] = ((train_set['tenure'] / 12) * (train_set['employ']) ).astype('float32')\n",
    "\n",
    "# Stability Feature (The Master Feature)\n",
    "factors = train_set['address'] + train_set['age'] + train_set['employ']\n",
    "train_set['stability_full'] = (factors * (train_set['tenure'] / 12)).astype('float32')\n",
    "\n",
    "# Good Score \n",
    "train_set['good_score'] = train_set[['callcard', 'confer', 'callwait']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f4ed2f-a6de-4913-94ea-cb045b9e39cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_features =  [ 'log_income', 'log_longmon', 'log_cardmon',\n",
    "       'log_wiremon', 'total_spend', 'affordability_idx', 'toxic_score',\n",
    "       'toxic_ed', 'ternure_longmon', 'ternure_cardmon', 'stability_age',\n",
    "       'stability_address', 'stability_employ', 'stability_full',\n",
    "       'good_score', 'churn',]\n",
    "GraphicsData(data = train_set[new_features]).numerical_histograms(hue = 'churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36af09ff-c30b-41f5-b494-80bef52a84f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Features Selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "705b3d92-9837-4149-b085-6f2e28e0343b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Numerical Features\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa58a755-83f8-498a-975e-dfd480a818ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Mann-Whitney U Test\n",
    "---\n",
    "- For the Feature Selection stage, I adopted the Mann-Whitney U Test. This choice is due to the high skewness (> 1) of the numerical variables, which requires a non-parametric approach robust to outliers, where traditional tests (such as the T-test) would fail.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8c5be3-e12f-4cad-ac84-82064bb21e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================= #\n",
    "# >>> Module of functions and classes for creating testing data. #                                        \n",
    "# ============================================================================= #\n",
    "\n",
    "# ======================================================== #\n",
    "# Imports:                                                 #\n",
    "# ======================================================== #\n",
    "# Data manipulation and testing:\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "# Scipy\n",
    "from scipy.stats import mannwhitneyu, pointbiserialr, chi2_contingency\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "class EDATest:\n",
    "\n",
    "# Initialize Class\n",
    "    def __init__(\n",
    "        self, \n",
    "        data: pd.DataFrame,\n",
    "    ):\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if data.empty:\n",
    "                raise ValueError('The provided DataFrame is empty.')\n",
    "\n",
    "            if data.isnull().any().any():\n",
    "                raise ValueError('The DataFrame contains null data.')\n",
    "\n",
    "            self.data = data\n",
    "\n",
    "        except Exception  as e:\n",
    "                print(f'[Error] Failed to load Dataframe : {str(e)}')\n",
    "\n",
    "\n",
    "    def mannwhitney_u_test(\n",
    "        self,\n",
    "        audit_vars: list,\n",
    "        target: str,\n",
    "    ):\n",
    "        \n",
    "        try:\n",
    "\n",
    "            results_audit = []\n",
    "\n",
    "            for var in audit_vars:\n",
    "\n",
    "                if var not in self.data.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Separating the groups\n",
    "                group_churn = self.data[self.data[target] == 1][var]\n",
    "                group_no_churn = self.data[self.data[target] == 0][var]\n",
    "                \n",
    "                # Mann-Whitney U Test\n",
    "                try:\n",
    "                    stat, p_val = mannwhitneyu(group_churn, group_no_churn, alternative = 'two-sided')\n",
    "                \n",
    "                except:\n",
    "                    p_val = 1.0\n",
    "                \n",
    "                # Linear Correlation (Point Biserial)\n",
    "                try:\n",
    "                    corr, _ = pointbiserialr(self.data[target], self.data[var])\n",
    "                \n",
    "                except:\n",
    "                    corr = 0\n",
    "\n",
    "                # Effect Size (Median Lift)\n",
    "                med_churn = group_churn.median()\n",
    "                med_loyal = group_no_churn.median()\n",
    "                # Calculete of lift_median\n",
    "                if med_loyal == 0:\n",
    "                    lift_median = med_churn\n",
    "                else:\n",
    "                    lift_median = ((med_churn - med_loyal) / med_loyal) * 100\n",
    "                \n",
    "                # Diagnosis/ Classification\n",
    "                if p_val < 0.001:\n",
    "                    strength = '⭐⭐⭐ Strong'\n",
    "\n",
    "                elif p_val < 0.05:\n",
    "                    strength =  '⭐⭐ Medium'\n",
    "                else:\n",
    "                    strength = '❌ Noise'\n",
    "                \n",
    "                # Results\n",
    "                results_audit.append({\n",
    "                    'Feature': var,\n",
    "                    'Strength': strength,\n",
    "                    'P-Value': round(p_val, 5),\n",
    "                    'Pearson Corr': round(corr, 3),\n",
    "                    'Median Churn': round(med_churn, 2),\n",
    "                    'Median Loyal': round(med_loyal, 2),\n",
    "                    'Impact (%)': round(lift_median, 1), \n",
    "                })\n",
    "            \n",
    "            # Sort by Absolute Correlation (to see who has the greatest impact, positive or negative)\n",
    "            df_audit = pd.DataFrame(results_audit)\n",
    "            df_audit['Abs_Corr'] = df_audit['Pearson Corr'].abs()\n",
    "            df_audit = df_audit.sort_values(by = 'Abs_Corr', ascending = False).drop(columns = ['Abs_Corr'])\n",
    "        \n",
    "            # Display\n",
    "            print('Table of Scientific Evidence:')\n",
    "            display(df_audit)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'[Error] Failure to generate statistical tests: {str(e)}')\n",
    "    \n",
    "\n",
    "    def vif_test(\n",
    "        self,\n",
    "        audit_vars: list,\n",
    "    ):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # Selection features for test\n",
    "            data = self.data[audit_vars].copy()\n",
    "\n",
    "            for col in data.columns:\n",
    "                # Ensures that booleans (True/False) reach 1/0\n",
    "                if data[col].dtype == 'bool':\n",
    "                    data[col] =  data[col].astype(int)\n",
    "                \n",
    "                # Force the numerical transformation to avoid errors in the test\n",
    "                data[col] = pd.to_numeric(data[col], errors = 'coerce')\n",
    "            # Cleaning data\n",
    "            data.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "            data.dropna(inplace = True)\n",
    "            print(f'Rows remaining after cleaning: {len(data)}')\n",
    "\n",
    "            # Calculating the VIF\n",
    "            data = data.astype(float)\n",
    "            x_vif = add_constant(data)\n",
    "\n",
    "            vif_data = pd.DataFrame()\n",
    "            vif_data['Feature'] = x_vif.columns\n",
    "            vif_data['VIF'] = [variance_inflation_factor(x_vif.values, i) for i in range(len(x_vif.columns))]   \n",
    "\n",
    "            print('\\n----- VIF Result -----') \n",
    "            print(vif_data.sort_values('VIF', ascending = False))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'[Error] Failure to generate VIF test: {str(e)}')\n",
    "    \n",
    "    def _cramers_v(\n",
    "        self,\n",
    "        x,\n",
    "        y\n",
    "    ):\n",
    "        try:\n",
    "\n",
    "            confusion_matrix = pd.crosstab(x, y)\n",
    "            if confusion_matrix.shape[0] < 2 or confusion_matrix.shape[1] < 2:\n",
    "                return 0 \n",
    "            chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "            n = confusion_matrix.sum().sum()\n",
    "            phi2 = chi2 / n\n",
    "            r, k = confusion_matrix.shape\n",
    "            with np.errstate(divide = 'ignore', invalid = 'ignore'):\n",
    "                phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "                rcorr = r - ((r - 1) **2) / (n - 1)\n",
    "                kcorr = k - ((k - 1) **2) / (n - 1)\n",
    "                if min((kcorr - 1), (rcorr - 1)) == 0:\n",
    "                    return 0\n",
    "                return np.sqrt(phi2corr / min((kcorr -1), (rcorr -1)))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'[Error] Failed to execute the function _cramers_v: {str(e)}')\n",
    "    \n",
    "    def chi_square_test(\n",
    "        self, \n",
    "        audit_vars: list,\n",
    "        target: str\n",
    "    ):\n",
    "        try:\n",
    "\n",
    "            # List of result\n",
    "            results_audit = []\n",
    "\n",
    "            for var in audit_vars:\n",
    "                # Check for existence\n",
    "                if var not in self.data.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Contingency Table (Crossover)\n",
    "                crosstab = pd.crosstab(self.data[var], self.data[target])\n",
    "\n",
    "                # Statistical Test (Chi-Square)\n",
    "                chi2, p_val, dof, expected = chi2_contingency(crosstab)\n",
    "\n",
    "                # Strength of Association (Cramér's V replaces Pearson)\n",
    "                assoc = self._cramers_v(self.data[var], self.data[target])\n",
    "                \n",
    "                # Calculates the churn rate for each category\n",
    "                # Axis 1 sums (Churn 0 + Churn 1) to get the category total\n",
    "                churn_rates = crosstab[1] / crosstab.sum(axis = 1)\n",
    "\n",
    "                # Indetify the extremes\n",
    "                # Category with the most churn\n",
    "                risky_cat = churn_rates.idxmax()                \n",
    "                # Rate %\n",
    "                risky_rate = churn_rates.max() * 100\n",
    "                \n",
    "                # Category with the least churn\n",
    "                safe_cat = churn_rates.idxmin()                \n",
    "                # Rate %\n",
    "                safe_rate = churn_rates.min() * 100\n",
    "\n",
    "                # Impact: The difference in risk between the worst and best-case scenarios\n",
    "                risk_gap = risky_rate - safe_rate\n",
    "                \n",
    "                # Diagnosis/ Classification\n",
    "                if expected.min() < 5:\n",
    "                    strength = '⚠️ Low sample'\n",
    "                elif p_val < 0.001:\n",
    "                    strength = '⭐⭐⭐ Strong'\n",
    "                elif p_val < 0.05:\n",
    "                    strength = '⭐⭐ Median'\n",
    "                else:\n",
    "                    strength = '❌ Noise'\n",
    "\n",
    "                # Results\n",
    "                results_audit.append({\n",
    "                    'Feature': var,\n",
    "                    'Strength': strength,\n",
    "                    'P-Value': round(p_val, 5),\n",
    "                    'Assoc. (Cramer V)': round(assoc, 3),\n",
    "                    'Worst Category': f'{risky_cat} ({risky_rate:.0f}%)',\n",
    "                    'Best Category': f'{safe_cat} ({safe_rate:.0f}%)',\n",
    "                    'Impact (Gap %)': round(risk_gap, 1)\n",
    "                })\n",
    "\n",
    "            # Ordering by Absolute Association (Cramér's V)\n",
    "            df_audit_cat = pd.DataFrame(results_audit)\n",
    "            df_audit_cat = df_audit_cat.sort_values(by = 'Assoc. (Cramer V)', ascending = False)\n",
    "\n",
    "            # Display\n",
    "            print('Table of Scientific Evidence (Categorical):')\n",
    "            display(df_audit_cat)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'[Error] Failure to generate statistical tests: {str(e)}')   \n",
    "\n",
    "    \n",
    "    def _correlation_ratio(\n",
    "        self,\n",
    "        categories, \n",
    "        measurements\n",
    "    ):\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Forcing and ensuring correct typing\n",
    "            cat = pd.Series(categories).astype(str)\n",
    "            meas = pd.to_numeric(measurements, errors = 'coerce')\n",
    "\n",
    "            # Security Cleanup (Removes NaNs generated during conversion)\n",
    "            # Creates a mask where both data sets are valid\n",
    "            valid_mask = (~cat.isna()) & (~meas.isna())\n",
    "            cat = cat[valid_mask]\n",
    "            meas = meas[valid_mask]\n",
    "\n",
    "            if len(meas) == 0: return 0.0\n",
    "\n",
    "            # Calcule of means\n",
    "            y_total_avg = meas.mean()\n",
    "            y_avg_per_cat = meas.groupby(cat, observed = True).mean()\n",
    "\n",
    "            # Maps the category average back to each observation.\n",
    "            y_avg_array = cat.map(y_avg_per_cat)\n",
    "\n",
    "            # Calcule of Variance (Eta)\n",
    "            numerator = np.sum((y_avg_array - y_total_avg) ** 2)\n",
    "            denominator = np.sum((meas - y_total_avg) ** 2)\n",
    "\n",
    "            if denominator == 0: return 0.0\n",
    "            return np.sqrt(numerator / denominator)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'[Error] Failed to execute the _correlation_ratio: {str(e)}')\n",
    "\n",
    "    def mixed_redundancy_test(\n",
    "        self,\n",
    "        audit_pairs: list,\n",
    "    ):\n",
    "        \n",
    "        try: \n",
    "\n",
    "            audit_results = []\n",
    "\n",
    "            for cat_col, num_col in audit_pairs:\n",
    "                if cat_col in self.data.columns and num_col in self.data.columns:\n",
    "\n",
    "                    # Executes the mixed redundancy test\n",
    "                    eta_score = self._correlation_ratio(self.data[cat_col], self.data[num_col])\n",
    "\n",
    "                    # Rules of decison\n",
    "                    decision = '✅ Keep'\n",
    "                    action = '-'\n",
    "\n",
    "                    if eta_score > 0.95:\n",
    "                        decision = '🔴 CRITIC (Redundant)'\n",
    "                        action = f'Dropt to `{cat_col}` or `{num_col}`'\n",
    "                    \n",
    "                    elif eta_score > 0.80:\n",
    "                        decision = '⚠️ Alert (Strong)'\n",
    "                        action = f'Evaluate removal'\n",
    "\n",
    "                    # Results\n",
    "                    audit_results.append({\n",
    "                        'Categorical (X)': cat_col,\n",
    "                        'Numerical (Y)': num_col,\n",
    "                        'Eta Score':  round(eta_score, 4),\n",
    "                        'Diagnosis' : decision,\n",
    "                        'Action': action\n",
    "                    })\n",
    "            \n",
    "            # Display\n",
    "            df_audit = pd.DataFrame(audit_results).sort_values(by = 'Eta Score', ascending = False)\n",
    "            display(df_audit)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'[Error] Failure to generate statistical tests: {str(e)}')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b51c8ea-c6d3-4f15-b44d-5ecac1e8965a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"#row_number#\":32},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770756158883}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_vars = [\n",
    "    'tenure', 'age', 'address', 'income', 'employ', \n",
    "    'longmon', 'tollmon', 'equipmon', 'cardmon',\n",
    "    'wiremon','log_income', 'log_longmon', 'log_cardmon',\n",
    "    'log_wiremon', 'total_spend', 'affordability_idx', 'toxic_score',\n",
    "    'toxic_ed', 'ternure_longmon', 'ternure_cardmon', 'stability_age',\n",
    "    'stability_address', 'stability_employ', 'stability_full',\n",
    "    'good_score'  \n",
    "]\n",
    "EDATest(data = train_set).mannwhitney_u_test(audit_vars =  audit_vars, target = 'churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db430a83-69d9-4259-91df-9fa2d2be68ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Conclusion of Test\n",
    "---\n",
    "- In this selection stage, variables with a P-value greater than 0.05 in the Mann-Whitney U test were excluded, indicating a lack of statistical significance. The case of the variable `income` is illustrative: even after the logarithmic transformation (`log_income`), the variable failed to reject the null hypothesis, proving that purchasing power is not a relevant discriminator of churn for this base.\n",
    "\n",
    "- **In contrast**, the analysis robustly validated that retention is governed by **stability factors** (tenure, age, address) and by **service quality** (toxicity), which demonstrated a direct and statistically significant influence on customer retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a90c89-5fa7-45b9-a3a7-e2cbaa9b32f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_vars = [\n",
    "    'tenure', 'age', 'address', 'employ', \n",
    "    'longmon', 'equipmon', 'cardmon',\n",
    "    'wiremon', 'log_longmon', 'log_cardmon',\n",
    "    'log_wiremon', 'toxic_score',\n",
    "    'toxic_ed', 'ternure_longmon', 'ternure_cardmon', 'stability_age',\n",
    "    'stability_address', 'stability_employ', 'stability_full', 'churn'\n",
    "]\n",
    "GraphicsData(train_set[audit_vars]).correlation_heatmap(v_size = 15, h_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96951390-c211-40dc-a04f-3f961557a6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Note:\n",
    "---\n",
    "- The creation of interaction variables generated mathematical redundancy in the dataset (multicollinearity). To mitigate this, I adopted the **'Kill the Parent'** strategy: remove the original variables when their derived versions ('children') show greater suitability to the Target.\n",
    "\n",
    "- As the new features better capture customer behavior, the following original variations were altered and removed to avoid noise in the model:\n",
    "\n",
    "- *Cut-off list:* `tenure`, `age`, `address`, `employ`, `longmon`, `cardmon`, `log_longmon`, `log_cardmon`, `logwiremon`, `toxic_score`.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e639d7e0-f51e-46cf-8bfe-2ce35f5c8bd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Multicollinearity Diagnosis (VIF Analysis)\n",
    "---\n",
    "- During the *Feature Engineering* phase, multiplicative interaction variables were created (e.g., `tenure` * `longmon`) to capture the elasticity of customer behavior over time. However, the introduction of these derived variables generates, by definition, a high degree of linear correlation with their original variables (Linear Dependence).\n",
    "\n",
    "- To mitigate the risk of redundancy and ensure the parsimony of the model (Occam's Razor), I will apply the **VIF (Variance Inflation Factor)** test. The goal is to identify and remove variables with a VIF > 10, ensuring that the final model prioritizes the real *Feature Importance* and does not suffer from instability in the estimation of the coefficients.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29642c4a-52e7-408a-be2f-4cffa12a2020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_vars = [\n",
    "    'equipmon', 'wiremon', 'toxic_ed', 'ternure_longmon', 'ternure_cardmon', 'stability_age',\n",
    "    'stability_address', 'stability_employ', 'stability_full',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29ae166-cd51-47f7-b226-4f79b51101ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EDATest(train_set).vif_test(audit_vars = audit_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8e16a05-72d1-45af-a428-501c00ac4988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_vars = [\n",
    "    'equipmon', 'wiremon', 'toxic_ed', 'ternure_longmon', 'ternure_cardmon', 'stability_age',\n",
    "    'stability_address', 'stability_employ', #'stability_full',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d77ba2-6f7d-4339-ad1b-69477cb98959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EDATest(train_set).vif_test(audit_vars = audit_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba5782b-6a28-4330-9138-424b3e8e90c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Note:\n",
    "---\n",
    "- **Variance Inflation Factor (VIF)** analysis detected severe multicollinearity in the derived feature `stability_full` (**VIF: 149.18**), indicating mathematical redundancy with its component variables (`stability_age`, `address`, `employ`).\n",
    "\n",
    "  **Action:** Removal of the variable `stability_full`.\n",
    "\n",
    "  **Result:** After deletion, the system rebalanced, with all remaining features showing **VIF < 10**.\n",
    "\n",
    "- Although `stability_full` showed a high linear correlation with the target, I chose to keep the component variables (such as `stability_employ`). These demonstrated a superior **Impact (Class Separation)** and offer greater granularity for decision-making. For Decision Tree/Ensemble algorithms, the ability for pure segregation (Information Gain) surpasses the aggregate linear correlation.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3e7454a-6753-48bc-877b-64164a34abb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3e90e3b-a43c-4791-8269-bd1391cdc208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Chi Square Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7214a94a-fe84-4509-a8a9-8f885b444216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_vars = [\n",
    "    'ed', 'equip', 'callcard', 'wireless', 'voice', 'pager', 'internet',\n",
    "    'callwait', 'confer', 'ebill', 'custcat'\n",
    "]\n",
    "EDATest(train_set).chi_square_test(audit_vars = audit_vars, target = 'churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c4237d-5556-4896-951e-824dbf361610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_set['ed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a00875d-c416-4aec-8e90-d2b942f63e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_set['ed'] = train_set['ed'].astype('int64').replace({5: 4})\n",
    "train_set['ed'] = train_set['ed'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44bb04f-5129-48af-811d-7fd4bb9c6974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EDATest(train_set).chi_square_test(audit_vars = audit_vars, target = 'churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f82862c-8d8f-403b-a5d2-0e3fd520cef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking mixed correlations of categorical features and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2953fa1d-d645-4371-ad7c-d99f7c5c2be4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_pairs = [\n",
    "    ('equip', 'equipmon'),       \n",
    "    ('wireless', 'wiremon'),     \n",
    "    ('callcard', 'ternure_cardmon'),     \n",
    "    ('ed', 'toxic_ed'),         \n",
    "    ('internet', 'toxic_ed'),\n",
    "    ('ebill', 'stability_age')\n",
    "]\n",
    "EDATest(train_set).mixed_redundancy_test(audit_pairs = audit_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b76ff6c-f22d-42f6-9d84-e77ddcb9418f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Data Preparation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c93a501-d27e-4e3b-9b24-c7307760377c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Selecting variables for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "570dc8a0-0ced-4289-a8e1-d2c0058a69f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_set[['ENGINESIZE','FUELCONSUMPTION_COMB']]\n",
    "y_train = train_set['CO2EMISSIONS']\n",
    "\n",
    "print(f'The shape of X_train is: {X_train.shape}')\n",
    "print(f'\\nThe shape of y_train is: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f56e17-4bf5-48c8-9cb2-354101e1e358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_test = test_set[['ENGINESIZE','FUELCONSUMPTION_COMB']]\n",
    "y_test = test_set['CO2EMISSIONS']\n",
    "\n",
    "print(f'The shape of X_train is: {X_test.shape}')\n",
    "print(f'\\nThe shape of y_train is: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf69115-92e5-4034-b17c-44b3304a5b67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2faf3c-01df-40d0-8a7b-a5fdcfe30819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Note:\n",
    "---\n",
    "- For the preprocessing stage, opt for the application of the `PowerTransformer` (Yeo-Johnson method). This application, a parametric power transformation technique, evolves to **stabilize the variance** and approximate the distribution of predictors to a Normal (Gaussian) distribution. The method acts by mitigating the positive skewness (**long tail**) and correcting the **heteroscedasticity** identified in the `ENGINESIZE` variable, thus ensuring the fulfillment of the statistical predictions of the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da28d20-3b95-4bb9-bdce-50c997bb4149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --- 1. Definição das Variáveis (Feature Selection) ---\n",
    "\n",
    "# A. Variáveis para EXCLUIR (Redundantes/Derivadas)\n",
    "drop_cols = [\n",
    "    'longten', 'tollten', 'cardten', # Totais (Interação Preço x Tempo)\n",
    "    'lninc', 'loglong', 'logtoll', 'logturn', # Logs (Matemáticas)\n",
    "    'callwait', 'confer', 'ebill', # Opcionais: Se quiser simplificar o modelo (Mantenha se achar relevante)\n",
    "    'churn' # O Target jamais entra no X\n",
    "]\n",
    "\n",
    "# B. Variáveis Numéricas (Precisam de Z-Score)\n",
    "numeric_features = [\n",
    "    'tenure', 'age', 'address', 'income', 'employ', \n",
    "    'longmon', 'tollmon', 'equipmon', 'cardmon', 'wiremon'\n",
    "]\n",
    "\n",
    "# C. Variáveis Binárias (Já estão prontas: 0/1)\n",
    "binary_features = [\n",
    "    'equip', 'callcard', 'wireless', 'pager', 'internet', 'voice'\n",
    "]\n",
    "\n",
    "# D. Variáveis Categóricas/Ordinais (Precisam de Dummies)\n",
    "# 'custcat' e 'ed' são números que representam categorias\n",
    "categorical_features = ['ed', 'custcat'] \n",
    "\n",
    "# --- 2. Separação X e y ---\n",
    "\n",
    "# Garante que estamos usando apenas as colunas que sobraram após o filtro mental\n",
    "selected_features = numeric_features + binary_features + categorical_features\n",
    "\n",
    "X = df[selected_features]\n",
    "y = df['churn'] # O Target isolado\n",
    "\n",
    "# Split Estratificado (Mantém a proporção de Churn no Treino e Teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- 3. Construção do Pipeline Robusto ---\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Numéricas: Padronização\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        \n",
    "        # Categóricas: One-Hot (drop='first' remove a colinearidade)\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features),\n",
    "        \n",
    "        # Binárias: Passar direto\n",
    "        ('bin', 'passthrough', binary_features)\n",
    "    ],\n",
    "    verbose_feature_names_out=False # Mantém nomes limpos (ex: 'ed_2' em vez de 'cat__ed_2')\n",
    ")\n",
    "\n",
    "# Pipeline Final\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        solver='liblinear', # Ótimo para datasets pequenos/médios\n",
    "        penalty='l1',       # Lasso: Ajuda a zerar coeficientes inúteis (Feature Selection automático)\n",
    "        C=1.0,              # Inverso da força de regularização\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 4. Treinamento ---\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Feedback Visual\n",
    "print(\"Pipeline treinado com sucesso.\")\n",
    "print(f\"Total de Features de Entrada: {X.shape[1]}\")\n",
    "print(f\"Total de Coeficientes Gerados (após One-Hot): {len(model_pipeline.named_steps['classifier'].coef_[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea18830-3fba-4212-b838-693a270fe2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_scaler = PowerTransformer(method = 'yeo-johnson')\n",
    "y_scaler = PowerTransformer(method = 'yeo-johnson')\n",
    "\n",
    "# Train data\n",
    "X_train_preprocessed  = X_scaler.fit_transform(X_train)\n",
    "\n",
    "# Test data \n",
    "X_test_preprocessed = X_scaler.transform(X_test)\n",
    "\n",
    "# Label data\n",
    "y_train_preprocessed  = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Test data \n",
    "y_test_preprocessed = y_scaler.transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5f3773-3e73-4cac-847a-de600431f821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Train Data Preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53646bde-82f3-4d22-91f7-66e860c7a8a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_preprocessed, columns = X_scaler.get_feature_names_out(X_train.columns)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f993ea43-b75b-41da-9722-dab173e9349e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Test Data Preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b838caa-267c-492f-b657-d43acb36f91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_test_preprocessed, columns = X_scaler.get_feature_names_out(X_test.columns)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08a3d98-7ff0-43c3-b6bf-d306a44b1f8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3aab528-f9a4-4783-9ee1-9f1575635ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b53c96-92cd-4f79-81ad-05ad999ac59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create model and  K-Fold\n",
    "model = LinearRegression()\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state = 33)\n",
    "\n",
    "# Create Cross-Validation\n",
    "cv_results = cross_validate(\n",
    "    model, \n",
    "    X_train_preprocessed,\n",
    "    y_train_preprocessed,\n",
    "    cv = kfold,\n",
    "    scoring = 'r2',\n",
    "    return_estimator = True\n",
    ")\n",
    "\n",
    "# Extraction of coefficients\n",
    "coefs_list = []\n",
    "for estimator in cv_results['estimator']:\n",
    "    coefs_list.append(estimator.coef_.flatten())\n",
    "\n",
    "# Converts to a NumPy array for easier statistical analysis (Shape: [5, n_features])\n",
    "coefs_array = np.array(coefs_list)\n",
    "\n",
    "# Stability Calculation (Audit)\n",
    "coefs_mean = np.mean(coefs_array, axis = 0)\n",
    "coefs_std = np.std(coefs_array, axis = 0)\n",
    "\n",
    "# Metrics\n",
    "print(f'--- Performance Metrics ---')\n",
    "print(f'Mean R² {np.mean(cv_results['test_score']):.4f}')\n",
    "print(f'Std R²: {np.std(cv_results['test_score']):.4f}')\n",
    "\n",
    "print(f'\\n--- Stability Metrics (Coefficients) ---')\n",
    "feature_names = ['ENGINESIZE', 'FUELCONSUMPTION_COMB']\n",
    "df_stability = pd.DataFrame(\n",
    "    {\n",
    "        'Feature': feature_names,\n",
    "        'Mean Coef': coefs_mean,\n",
    "        'Std Coef': coefs_std,\n",
    "        'CV (%)': (coefs_std / np.abs(coefs_mean)) * 100\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df_stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbddd31-a65c-44a2-a174-9671fbb9f078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c3019a8-6640-4c1b-8c7e-194102db1800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_preprocessed, y_train_preprocessed)\n",
    "\n",
    "print(f'Coefficients: {model.coef_[0]}')\n",
    "print(f'Intercept: {model.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6518f4c5-da36-42cc-909b-e9dc6b36122a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e828638f-bf14-4530-9444-9ac9cecc63a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363849f1-7bc4-48ec-a5d3-3139954fce62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67991e81-aa40-4f2b-8d65-850191897f62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Bring the test Y and the predicted Y back to the \"Real World\"\n",
    "# The inverse_transform requires a 2D array, hence the reshape\n",
    "y_pred_real = y_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "y_test_real = y_scaler.inverse_transform(y_test_preprocessed)\n",
    "\n",
    "#2. Calculate metrics on the REAL scale (Grams of CO2)\n",
    "mae_real = mean_absolute_error(y_test_real, y_pred_real)\n",
    "rmse_real = root_mean_squared_error(y_test_real, y_pred_real)\n",
    "r2_real = r2_score(y_test_real, y_pred_real)\n",
    "\n",
    "print(f'--- Business Metrics (Original Scale) ---')\n",
    "print(f\"MAE Real: {mae_real:.2f} g/km\")\n",
    "print(f\"RMSE Real: {rmse_real:.2f} g/km\")\n",
    "print(f\"R2 Real: {r2_real:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Statistical Metrics (Yeo-Johnson Scale) ---\")\n",
    "print(f\"R2 Transformed: {r2_score(y_test_preprocessed, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d8dc0a-f052-496e-8a0f-e4557fa88699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Key Observations:\n",
    "---\n",
    "\n",
    "- 1. **Cross-Validation:** The application of Cross-Validation demonstrated exceptional stability in the model. The standard deviation of only **0.0170** between the k-folds confirms that the performance (average R² of **0.899**) is consistent and robust, minimizing the risk of sampling bias.\n",
    "---\n",
    "\n",
    "- 2. **Generalization Test:** In the test data, the model achieved a **Transformed R² of 0.91** (and **0.88** in the Real Scale). This transformed score, slightly higher than the training score (**0.90**), confirms that there was absolutely no *Overfitting*. The model learned the underlying physics of the data rather than memorizing noise.\n",
    "---\n",
    "- 3. **Stability (CV%):** **(`ENGINESIZE`)**: **4.05%** and **(`FUELCONSUMPTION_COMB`)**: **1.92%**. The CV is drastically below the 20% threshold, indicating \"State-of-the-Art\" stability. The **multicollinearity** (0.82 correlation) was effectively neutralized. The model assigned a clear, unwavering weight to the Fuel Consumption (Mean Coef ~0.71) as the dominant factor, while maintaining Engine Size (Mean Coef ~0.27) as a stable secondary predictor.\n",
    "---\n",
    "#### Insight:\n",
    "---\n",
    "---\n",
    "- The convergence between the training **R² (0.90)** and the transformed test **R² (0.91)** validates the **Yeo-Johnson** strategy. The slight decrease to **0.88** in the \"Real Scale\" is mathematically expected due to the non-linear inverse transformation of residuals, but it represents the honest accuracy for the business (MAE ~13g/km). Technically, the model successfully linearized a complex physical phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5163d02e-8d74-46c2-b17f-2c28dda6e442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d02198b-2650-4929-a4cb-12be0dffbc9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_lift_and_gains(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Gera a Tabela de Lift e os Gráficos de Gains/Lift.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Array com os valores reais (0 ou 1)\n",
    "        y_proba: Array com as probabilidades preditas (predict_proba)\n",
    "    \"\"\"\n",
    "    # 1. Criar DataFrame auxiliar\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_proba': y_proba})\n",
    "    \n",
    "    # 2. Ordenar por probabilidade (Ranking)\n",
    "    df = df.sort_values('y_proba', ascending=False)\n",
    "    \n",
    "    # 3. Criar os Decis (qcut divide em grupos de tamanho igual)\n",
    "    df['decile'] = pd.qcut(df['y_proba'].rank(method='first'), 10, labels=False)\n",
    "    df['decile'] = 10 - df['decile'] # Inverter para que 1 seja o Top Risk\n",
    "    \n",
    "    # 4. Agregação (A Mágica acontece aqui)\n",
    "    lift_table = df.groupby('decile')['y_true'].agg(['count', 'sum', 'mean']).reset_index()\n",
    "    lift_table.columns = ['Decile', 'Total_Customers', 'Real_Churners', 'Churn_Rate']\n",
    "    \n",
    "    # 5. Cálculos de Engenharia\n",
    "    global_churn_rate = df['y_true'].mean()\n",
    "    lift_table['Lift'] = lift_table['Churn_Rate'] / global_churn_rate\n",
    "    \n",
    "    # Cumulative Gains (Quanto do churn total eu peguei?)\n",
    "    lift_table['Cumulative_Churners'] = lift_table['Real_Churners'].cumsum()\n",
    "    lift_table['Total_Churners_In_Base'] = df['y_true'].sum()\n",
    "    lift_table['Gain_Percentage'] = lift_table['Cumulative_Churners'] / lift_table['Total_Churners_In_Base']\n",
    "    \n",
    "    # --- VISUALIZAÇÃO ---\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Gráfico de Barras (Lift por Decil)\n",
    "    sns.barplot(x='Decile', y='Lift', data=lift_table, color='skyblue', alpha=0.7, ax=ax1)\n",
    "    ax1.axhline(1, color='red', linestyle='--', label='Baseline (Aleatório)')\n",
    "    ax1.set_ylabel('Lift (x vezes melhor que aleatório)')\n",
    "    ax1.set_title('Lift Analysis & Cumulative Gains', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Gráfico de Linha (Ganho Acumulado) - Eixo Secundário\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(x=lift_table.index, y=lift_table['Gain_Percentage'], color='green', marker='o', ax=ax2, label='% Churn Capturado')\n",
    "    ax2.set_ylabel('% Total de Churners Capturados')\n",
    "    ax2.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Destaque do KPI \"Lift in Top Decile\"\n",
    "    top_decile_gain = lift_table.loc[0, 'Gain_Percentage']\n",
    "    plt.text(0, top_decile_gain, f'{top_decile_gain:.0%} Capturado', \n",
    "             bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return lift_table\n",
    "\n",
    "# Exemplo de chamada no seu notebook:\n",
    "# y_proba = model.predict_proba(X_test)[:, 1] # Pegar prob da classe 1\n",
    "# lift_df = plot_lift_and_gains(y_test, y_proba)\n",
    "# display(lift_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3635cd0c-220e-4a30-b402-804d44af36f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Flatten arrays to ensure 1D dimension.\n",
    "y_pred_real = y_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "y_test_real = y_scaler.inverse_transform(y_test_preprocessed).flatten()\n",
    "\n",
    "# Waste calculation\n",
    "residuals = y_test_real - y_pred_real\n",
    "\n",
    "# --- GRÁFICO A: Residuals vs Predicted ---\n",
    "\n",
    "# Subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize = (21, 7))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x = y_pred_real,\n",
    "    y = residuals,\n",
    "    ax = ax[0],\n",
    "    alpha = 0.6,\n",
    "    color = 'steelblue',\n",
    "    edgecolor = 'black',\n",
    "    s = 70\n",
    ")\n",
    "# Reference Line (Zero Error)\n",
    "ax[0].axhline(y = 0, color = 'crimson', linestyle = '--', linewidth = 2, label = 'Zero Error' )\n",
    "\n",
    "std_resid = np.std(residuals)\n",
    "ax[0].axhline(y = std_resid * 2, color = 'gray', linestyle = ':', alpha = 0.5, label = '+/- Std Dev')\n",
    "ax[0].axhline(y = -std_resid * 2, color = 'gray', linestyle = ':', alpha = 0.5 )\n",
    "\n",
    "ax[0].set_title('A. Homoscedasticity: Residuals vs. Predicted Values', fontsize = 14, fontweight = 'bold')\n",
    "ax[0].set_xlabel('Predicted Emission (g/km)', fontsize = 12)\n",
    "ax[0].set_ylabel('Error (Real - Predicted)', fontsize = 12)\n",
    "ax[0].legend()\n",
    "ax[0].grid(True, alpha = 0.3)\n",
    "\n",
    "\n",
    "# --- GRAPH B: Distribution of Residuals (The Normality Test)\n",
    "sns.histplot(\n",
    "    residuals, \n",
    "    kde = True, \n",
    "    ax = ax[1],\n",
    "    color = 'darkslategray',\n",
    "    edgecolor = 'black',\n",
    "   \n",
    ")\n",
    "\n",
    "mean_resid = np.mean(residuals)\n",
    "ax[1].axvline(mean_resid, color = 'gold', linestyle = '-', linewidth = 3, label = f'Mean Error: {mean_resid:.2f}')\n",
    "\n",
    "ax[1].set_title('B. Normality: Error of Distribution', fontsize = 14, fontweight = 'bold')\n",
    "ax[1].set_xlabel('Magnitude of Error (g/km)', fontsize = 12)\n",
    "ax[1].set_ylabel('Frequency', fontsize = 12)\n",
    "ax[1].legend()\n",
    "ax[1].grid(True, alpha = 0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7311148d-515d-4cfb-9759-86801eaca144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Key Observations:\n",
    "---\n",
    "\n",
    "#### 1. Technical Performance\n",
    "\n",
    "  - **Explanatory Power (Real R² Score): 0.8844**\n",
    "\n",
    "  - The model explains **88.4% of the variability** in CO2 emissions using the \"Real World\" scale (g/km). In the transformed mathematical space (Yeo-Johnson), the fit is even higher (**0.91**), confirming that the non-linear approach successfully captured the physical behavior of the data. This is a significant improvement over the simple univariate model (~0.80).\n",
    "\n",
    "  - **Margin of Error (MAE): 13.03**\n",
    "\n",
    "  - The **Mean Absolute Error** indicates that, on average, our predictions deviate by only **13.03 g/km** from the actual value. For a business context where emissions range up to 450 g/km, this represents a very high precision level (approx. 5% relative error), allowing for reliable carbon footprint estimation.\n",
    "\n",
    "  - **Sensitivity to Large Errors (RMSE vs MAE):**\n",
    "\n",
    "  * The **RMSE (20.87)** is controlled relative to the MAE (13.03). The gap of ~7.8 points is healthy. It indicates that while there are outliers (likely high-performance sports cars or heavy vehicles), the **Yeo-Johnson transformation** successfully mitigated the extreme penalties that usually distort linear models.\n",
    "---\n",
    "\n",
    "#### 2. Model Interpretation\n",
    "\n",
    "  - The model utilizes a **Power Law** approach (Yeo-Johnson) rather than a simple straight line:\n",
    "\n",
    "  - **Feature Dominance (Standardized Coefficients):**\n",
    "  - **Fuel Consumption (Coefficient ~0.71):** This is the dominant driver. The stability analysis showed a variation of only **1.92%** (CV) for this weight, proving it is the most reliable predictor.\n",
    "  - **Engine Size (Coefficient ~0.27):** This acts as a secondary adjustment factor. Even with a 0.82 correlation to fuel, the model successfully isolated its unique contribution with high stability (CV ~4.05%).\n",
    "\n",
    "  - **The \"Curved\" Surface:**\n",
    "  - Unlike a rigid linear equation, the model projects a **curved surface**. This means it understands that \"efficiency\" changes as engines get bigger. Physically, this represents the diminishing returns of combustion efficiency in larger engines, providing a much more realistic simulation than a simple linear slope.\n",
    "---\n",
    "\n",
    "#### 3. **Conclusion:**\n",
    "\n",
    "- The Multiple Regression Model with Power Transformation represents the \"State-of-the-Art\" for this dataset.\n",
    "\n",
    "- **Strengths:** - **Robustness:** The coefficient stability (CV < 5%) proves the model is immune to multicollinearity. \n",
    "- **Physical Coherence:** The residuals follow a near-perfect Gaussian distribution (Mean Bias ~0.81g), indicating that all deterministic signal has been captured.\n",
    "\n",
    "- **Limitations:**\n",
    "\n",
    "- **Interpretability:** Because the model operates in a transformed space, we cannot say \"1 liter adds X grams\" directly. We must use the inverse transformation to get real values.\n",
    "- **Scope:** The slight residual spread at the high end (>350g/km) suggests that for extreme heavy-duty vehicles, a separate model or additional features (like vehicle weight) might be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d12f08-73bb-4a99-9a37-b4df11e43014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Deployment:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18588a0f-aff5-4f4f-928b-c94ff4ae4cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DEPLOY PACKAGE: We save everything in a dictionary to ensure integrity.\n",
    "production_bundle = {\n",
    "    'model': model, \n",
    "    'pt_X': X_scaler, \n",
    "    'pt_y': y_scaler,\n",
    "}\n",
    "\n",
    "# Saved in a single \"pickle\" file\n",
    "joblib.dump(production_bundle, './artifacts/co2_pipeline_v2.pkl')\n",
    "\n",
    "# Return\n",
    "print('✅ Complete pipeline saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8c63f78-0794-4a24-b6b7-f08727102182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict_emission(engine_size, fuel_consumption):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs full inference with Yeo-Johnson pre- and post-processing.\n",
    "    \"\"\"\n",
    "    # 1. Loading (Load the Complete Package)\n",
    "    bundle = joblib.load('./artifacts/co2_pipeline_v2.pkl')\n",
    "    model_loaded = bundle['model']\n",
    "    pt_X_loaded = bundle['pt_X']\n",
    "    pt_y_loaded = bundle['pt_y']\n",
    "\n",
    "    # 2. Input Data Engineering\n",
    "    input_data = pd.DataFrame(\n",
    "        [[engine_size, fuel_consumption]],\n",
    "        columns = ['ENGINESIZE', 'FUELCONSUMPTION_COMB']\n",
    "    )\n",
    "\n",
    "    # 3. Pre-processing\n",
    "    input_transformed = pt_X_loaded.transform(input_data)\n",
    "\n",
    "    # 4. Prediction\n",
    "    prediction_transformed = model_loaded.predict(input_transformed)\n",
    "\n",
    "    # 5. Post-processing (Reducing to Grams of CO2)\n",
    "    prediction_real  = pt_y_loaded.inverse_transform(prediction_transformed.reshape(-1, 1))\n",
    "\n",
    "    result_g_km = prediction_real[0][0]\n",
    "\n",
    "    return result_g_km\n",
    "\n",
    "# --- FINAL TEST (User Acceptance Test) ---\n",
    "\n",
    "# Scenario: 2.0L car getting 8.5 L/100km\n",
    "engine = 2.0\n",
    "consumption = 8.5\n",
    "\n",
    "try:\n",
    "    prediction = predict_emission(engine, consumption)\n",
    "\n",
    "    print(f'--- INFERENCE REPORT ---')\n",
    "    print(f'Engine: {engine} L')\n",
    "    print(f'Consumption: {consumption} L/100km')\n",
    "    print(f'Predicted Emission: {prediction:.2f} g/km')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Deployment error: {e}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_Modeling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
