{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e0fdcbb-5814-4bd7-9323-99338b744b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Customer Churn - Telecom\n",
    "---\n",
    "\n",
    "### CRISP-DM Methodology\n",
    "This project follows the CRISP-DM (*Cross-Industry Standard Process for Data Mining*) framework applied to **Customer Retention & Churn Prediction**:\n",
    "| **Stage** | **Objective** | **Methodological Execution** |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. Business Understanding** | Mitigate revenue loss by identifying at-risk customers. | • **Target Definition**: Binary Classification (Churn: Yes/No).<br>• **KPIs**: Maximize **Lift** in retention campaigns & Revenue Saved vs. Cost. |\n",
    "| **2. Data Understanding** | Detect patterns of friction and dissatisfaction. | • **EDA**: Distribution analysis (Detect Imbalance).<br>• **Hypothesis Testing**: Correlation Matrix & Independence Tests (Chi-Square). |\n",
    "| **3. Data Preparation** | Construct a robust dataset for parametric modeling. | • **Scaling**: Standardization (Z-score) for coefficient comparability.<br>• **Encoding**: One-Hot Encoding for nominal variables.<br>• **Splitting**: Stratified Train/Test Split to preserve class ratio. |\n",
    "| **4. Modeling** | Estimate Churn Probability | • **Algorithms**: Logistic Regression, SVM Linear, KNN, Regression, Decision Tree, Random Florest, XGBoost, LightGBM.<br>• **Inference**: Analyze **Odds Ratios** to determine feature elasticity. |\n",
    "| **5. Evaluation** | Assess model reliability and financial impact. | • **Discrimination**: AUC-ROC & F1-Score (Threshold Tuning).<br>• **Calibration**: Probability Calibration Curve (Reliability Diagram). |\n",
    "| **6. Deployment** | Integrate insights into the CRM lifecycle. | • **Deliverable**: \"High-Risk\" Customer List for Marketing Squad.<br>• **Artifact**: Serialize model (`joblib`) for batch inference. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d932ccc9-9725-4c0b-a659-a24aa6f2dd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Installs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d3a5b4-e41a-4a61-bce4-4dceb05903fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -r '../requirements.txt'\n",
    "# Command to restart the kernel and update the installed libraries\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e9c6310-74b4-4ed9-a99e-8f757cc87e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff0c4ba1-0c69-492d-96f7-660939c88027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Analize and Visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Modeling / Model Linear / Metrics / Save Model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e5d7c5-6054-43cb-889f-6666de736ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SRC/ Functions Utils:\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from visualization import GraphicsData\n",
    "from utils import EDATest, optimize_dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0423f5ec-298e-4812-a2d4-489b479224a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dev objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f94f823-143c-4818-95a3-55ce8c888e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================= #\n",
    "# >>> Module of functions and classes for creating graphs and visualizing data. #                                        \n",
    "# ============================================================================= #\n",
    "\n",
    "# ======================================================== #\n",
    "# Imports:                                                 #\n",
    "# ======================================================== #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class FeatureEngineer(\n",
    "    BaseEstimator, \n",
    "    TransformerMixin\n",
    "):\n",
    "\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    # Initialize Class\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"\"\"\"\n",
    "\n",
    "    # Def Fit\n",
    "    def fit(\n",
    "        self, \n",
    "        X, \n",
    "        y = None\n",
    "    ):\n",
    "        \"\"\"\"\"\"\n",
    "\n",
    "    def transform(\n",
    "        self, \n",
    "        X\n",
    "    ):\n",
    "        \n",
    "        # Check Data set \n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        # -------- 1) Total wallet share --------\n",
    "        mon_cols = ['longmon', 'tollmon', 'equipmon', 'cardmon','wiremon']\n",
    "        existing_mon = [c for c in  mon_cols if c in X.columns]\n",
    "        X['total_spend'] = X[existing_mon].sum(axis = 1).astype('float32') if existing_mon else 0.0\n",
    "\n",
    "        # -------- 2) Affordability and expenses/income --------\n",
    "\n",
    "        if 'income' in X.columns:\n",
    "            income_scale = 1000.0\n",
    "            denom_income = X['income'].astype('float64') * income_scale + 1.0\n",
    "        else: \n",
    "            denom_income = 1.0\n",
    "\n",
    "        X['affordability_idx'] = (\n",
    "            X['total_spend'].astype('float64') / denom_income\n",
    "        ).astype('float32')\n",
    "\n",
    "        for col in ['longmon', 'equipmon', 'cardmon', 'wiremon']:\n",
    "            if col in X.columns:\n",
    "                X[f'{col}_inc'] = (\n",
    "                    X[col].astype('float64') / denom_income\n",
    "                ).astype('float32')\n",
    "            \n",
    "            else:\n",
    "                X[f'{col}_inc'] = 0.0\n",
    "        \n",
    "        # -------- 3) Risk (toxicity + education) --------\n",
    "        toxic_list = ['internet', 'wireless', 'equip', 'voice', 'pager']\n",
    "        existing_toxic = [c for c in toxic_list if c in X.columns]\n",
    "        X['toxic_score'] = X[existing_toxic].sum(axis = 1).astype('float32') if existing_toxic else 0.0\n",
    "\n",
    "        if 'ed' in X.columns:\n",
    "            X['toxic_ed'] = (\n",
    "                X['toxic_score'].astype('float64') * X['ed'].astype('int64')\n",
    "            ).astype('float32')\n",
    "        else:\n",
    "            X['toxic_ed'] = 0.0\n",
    "\n",
    "        # -------- 4) Tenure in years (tenure is in months) --------\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "447c95e8-1778-41f4-9b89-ac5daa9e15d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        # -------- 4) Tenure em anos (tenure está em meses) --------\n",
    "        if \"tenure\" in X.columns:\n",
    "            X[\"tenure_years\"] = (\n",
    "                X[\"tenure\"].astype(\"float64\") / 12.0\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"tenure_years\"] = 0.0\n",
    "\n",
    "        # -------- 5) Comportamento de uso --------\n",
    "        # tenure_longmon / tenure_cardmon\n",
    "        if \"longmon\" in X.columns:\n",
    "            X[\"tenure_longmon\"] = (\n",
    "                X[\"tenure_years\"].astype(\"float64\")\n",
    "                * X[\"longmon\"].astype(\"float64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"tenure_longmon\"] = 0.0\n",
    "\n",
    "        if \"cardmon\" in X.columns:\n",
    "            X[\"tenure_cardmon\"] = (\n",
    "                X[\"tenure_years\"].astype(\"float64\")\n",
    "                * X[\"cardmon\"].astype(\"float64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"tenure_cardmon\"] = 0.0\n",
    "\n",
    "        # age_longmon / age_cardmon (age em anos, SEM /12)\n",
    "        if \"age\" in X.columns and \"longmon\" in X.columns:\n",
    "            X[\"age_longmon\"] = (\n",
    "                X[\"age\"].astype(\"float64\") * X[\"longmon\"].astype(\"float64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"age_longmon\"] = 0.0\n",
    "\n",
    "        if \"age\" in X.columns and \"cardmon\" in X.columns:\n",
    "            X[\"age_cardmon\"] = (\n",
    "                X[\"age\"].astype(\"float64\") * X[\"cardmon\"].astype(\"float64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"age_cardmon\"] = 0.0\n",
    "\n",
    "        # -------- 6) Estabilidade --------\n",
    "        if \"age\" in X.columns:\n",
    "            X[\"stability_age\"] = (\n",
    "                X[\"tenure_years\"].astype(\"float64\")\n",
    "                * (X[\"age\"].astype(\"float64\") - 18.0)\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"stability_age\"] = 0.0\n",
    "\n",
    "        if \"address\" in X.columns:\n",
    "            X[\"stability_address\"] = (\n",
    "                X[\"tenure_years\"].astype(\"float64\")\n",
    "                * X[\"address\"].astype(\"float64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"stability_address\"] = 0.0\n",
    "\n",
    "        if \"employ\" in X.columns:\n",
    "            X[\"stability_employ\"] = (\n",
    "                X[\"tenure_years\"].astype(\"float64\")\n",
    "                * X[\"employ\"].astype(\"float64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"stability_employ\"] = 0.0\n",
    "\n",
    "        # -------- 7) Good score --------\n",
    "        good_cols = [\"callcard\", \"confer\", \"callwait\"]\n",
    "        existing_good = [c for c in good_cols if c in X.columns]\n",
    "        X[\"good_score\"] = X[existing_good].sum(axis=1) if existing_good else 0.0\n",
    "\n",
    "        # -------- 8) Limpeza final (inf / NaN -> 0), igual ao estilo do projeto --------\n",
    "        X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e63f7d4-0d85-4c27-b479-920491c73e13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeatureEngineerTelecom(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Engenharia de features para o dataset de telecom.\n",
    "\n",
    "    - Agregações de custos (total_spend)\n",
    "    - Razões gasto / renda (affordability_idx, longmon_inc, etc.)\n",
    "    - Risco (toxic_score, toxic_ed)\n",
    "    - Comportamento de uso (tenure/age x gastos)\n",
    "    - Estabilidade (tenure x perfil)\n",
    "    - Good score\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, income_in_thousands: bool = True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        income_in_thousands : bool, default=True\n",
    "            Se True, assume que a coluna 'income' está em milhares (20 = 20.000)\n",
    "            e multiplica por 1000 no denominador das razões.\n",
    "        \"\"\"\n",
    "        self.income_in_thousands = income_in_thousands\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Nada a aprender; apenas compatível com API do sklearn\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Garante DataFrame (caso venha como array)\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        # -------- 1) Total wallet share --------\n",
    "        mon_cols = [\"longmon\", \"tollmon\", \"equipmon\", \"cardmon\", \"wiremon\"]\n",
    "        existing_mon = [c for c in mon_cols if c in X.columns]\n",
    "        X[\"total_spend\"] = X[existing_mon].sum(axis=1) if existing_mon else 0.0\n",
    "\n",
    "        # -------- 2) Affordability e gastos/renda --------\n",
    "        if \"income\" in X.columns:\n",
    "            income_scale = 1000.0 if self.income_in_thousands else 1.0\n",
    "            denom_income = X[\"income\"].astype(\"float64\") * income_scale + 1.0\n",
    "        else:\n",
    "            denom_income = 1.0\n",
    "\n",
    "        X[\"affordability_idx\"] = (\n",
    "            X[\"total_spend\"].astype(\"float64\") / denom_income\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        for col in [\"longmon\", \"equipmon\", \"cardmon\", \"wiremon\"]:\n",
    "            if col in X.columns:\n",
    "                X[f\"{col}_inc\"] = (\n",
    "                    X[col].astype(\"float64\") / denom_income\n",
    "                ).astype(\"float32\")\n",
    "            else:\n",
    "                X[f\"{col}_inc\"] = 0.0\n",
    "\n",
    "        # -------- 3) Risco (toxicidade + educação) --------\n",
    "        toxic_list = [\"internet\", \"wireless\", \"equip\", \"voice\", \"pager\"]\n",
    "        existing_toxic = [c for c in toxic_list if c in X.columns]\n",
    "        X[\"toxic_score\"] = X[existing_toxic].sum(axis=1) if existing_toxic else 0.0\n",
    "\n",
    "        if \"ed\" in X.columns:\n",
    "            X[\"toxic_ed\"] = (\n",
    "                X[\"toxic_score\"].astype(\"float64\") * X[\"ed\"].astype(\"int64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"toxic_ed\"] = 0.0\n",
    "\n",
    "        # -------- 4) Tenure em anos (tenure está em meses) --------\n",
    "        if \"tenure\" in X.columns:\n",
    "            X[\"tenure_years\"] = (\n",
    "                X[\"tenure\"].astype(\"float64\") / 12.0\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"tenure_years\"] = 0.0\n",
    "\n",
    "        # -------- 5) Comportamento de uso --------\n",
    "        # tenure_longmon / tenure_cardmon\n",
    "        if \"longmon\" in X.columns:\n",
    "            X[\"tenure_longmon\"] = (\n",
    "                X[\"tenure_years\"].astype(\"float64\")\n",
    "                * X[\"longmon\"].astype(\"float64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"tenure_longmon\"] = 0.0\n",
    "\n",
    "        if \"cardmon\" in X.columns:\n",
    "            X[\"tenure_cardmon\"] = (\n",
    "                X[\"tenure_years\"].astype(\"float64\")\n",
    "                * X[\"cardmon\"].astype(\"float64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"tenure_cardmon\"] = 0.0\n",
    "\n",
    "        # age_longmon / age_cardmon (age em anos, SEM /12)\n",
    "        if \"age\" in X.columns and \"longmon\" in X.columns:\n",
    "            X[\"age_longmon\"] = (\n",
    "                X[\"age\"].astype(\"float64\") * X[\"longmon\"].astype(\"float64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"age_longmon\"] = 0.0\n",
    "\n",
    "        if \"age\" in X.columns and \"cardmon\" in X.columns:\n",
    "            X[\"age_cardmon\"] = (\n",
    "                X[\"age\"].astype(\"float64\") * X[\"cardmon\"].astype(\"float64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"age_cardmon\"] = 0.0\n",
    "\n",
    "        # -------- 6) Estabilidade --------\n",
    "        if \"age\" in X.columns:\n",
    "            X[\"stability_age\"] = (\n",
    "                X[\"tenure_years\"].astype(\"float64\")\n",
    "                * (X[\"age\"].astype(\"float64\") - 18.0)\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"stability_age\"] = 0.0\n",
    "\n",
    "        if \"address\" in X.columns:\n",
    "            X[\"stability_address\"] = (\n",
    "                X[\"tenure_years\"].astype(\"float64\")\n",
    "                * X[\"address\"].astype(\"float64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"stability_address\"] = 0.0\n",
    "\n",
    "        if \"employ\" in X.columns:\n",
    "            X[\"stability_employ\"] = (\n",
    "                X[\"tenure_years\"].astype(\"float64\")\n",
    "                * X[\"employ\"].astype(\"float64\")\n",
    "            ).astype(\"float32\")\n",
    "        else:\n",
    "            X[\"stability_employ\"] = 0.0\n",
    "\n",
    "        # -------- 7) Good score --------\n",
    "        good_cols = [\"callcard\", \"confer\", \"callwait\"]\n",
    "        existing_good = [c for c in good_cols if c in X.columns]\n",
    "        X[\"good_score\"] = X[existing_good].sum(axis=1) if existing_good else 0.0\n",
    "\n",
    "        # -------- 8) Limpeza final (inf / NaN -> 0), igual ao estilo do projeto --------\n",
    "        X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47872334-51bc-4c5f-8a01-89b60093cc53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae624b9-31dc-49c2-9031-1e522bf51f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ChurnData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197d306c-26b3-4f50-89c6-eae69daba922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verify successful load with some randomly selected records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b87af9-26b9-4310-9a69-0e44024f61c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.sample(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56ef6fc-9d60-45a3-aadf-473c9ef4902d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cfb6420-fef4-43a3-ac20-9352c4b90c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "425be8a4-841d-4c94-ad30-5f5e314863cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99781e41-3c66-402a-ae89-2547934fdc0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Adjusting the variable types with their respective characteristics. \n",
    "---\n",
    "- In this data, there are both binary and ordinal variables; I will be adjusting them so that there is no invalid statistical aggregation in the analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "453c947b-3378-4ba1-864b-f2d77dc49de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = optimize_dtypes(df)\n",
    "\n",
    "print(f'New dtypes of variables:')\n",
    "df.info()\n",
    "\n",
    "print(f'Visual sample:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b5ce6e-c406-4f1a-b70d-b2fd053af03b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GraphicsData(data = df).plot_target_analysis(target_col='churn', colors=['#1abc9c', '#ff6b6b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9230454d-a717-4798-a0ae-fec0fa273bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Train and Test Data Split\n",
    "---\n",
    "\n",
    "- Before starting the data preprocessing and modeling process, the dataset will be split into **Training** and **Test** sets.\n",
    "\n",
    "The main goal is to prevent data leakage, ensuring that all statistical information, outlier handling, transformation decisions, and feature engineering strategies are derived exclusively from the training data.\n",
    "\n",
    "> This approach preserves the integrity of the validation process and ensures that performance metrics reflect the model’s true generalization capability, rather than contamination from information in the test set.\n",
    "\n",
    "- The `stratify` parameter will be applied in the `train_test_split` procedure.\n",
    "\n",
    "Since churn prediction is a binary classification problem, keeping the original class proportion in both subsets is statistically recommended and, in practice, helps avoid evaluation bias.\n",
    "\n",
    "> Stratified sampling preserves the prior distribution of the target variable, reducing distortions in class balance that could bias model training, decision-threshold calibration, and metrics such as Recall, Precision, and ROC-AUC.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a623dd8-9a20-4296-9b0f-673e363ab7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df, test_size = 0.2, stratify = df['churn'], shuffle = True, random_state = 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4530052b-e86b-463c-b034-363f0dc9ba84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking the proportions of the target variable\n",
    "print(f'Shape of training: {train_set.shape}')\n",
    "print(f'Shape of test: {test_set.shape}')\n",
    "\n",
    "print('\\n--- Churn Rate (Stratify Validation) ---')\n",
    "print(f'Original: {df['churn'].mean():.2%}')\n",
    "print(f'Train:    {train_set['churn'].mean():.2%}')\n",
    "print(f'Test:    {test_set['churn'].mean():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f9c6d49-030d-4b6b-95f4-dfc18fc1c277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def engineer_features(df: pd.DataFrame, *, income_in_thousands: bool = True) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "\n",
    "    # 1) total_spend\n",
    "    mon_cols = [\"longmon\", \"tollmon\", \"equipmon\", \"cardmon\", \"wiremon\"]\n",
    "    X[\"total_spend\"] = X[[c for c in mon_cols if c in X.columns]].sum(axis=1)\n",
    "\n",
    "    # 2) renda no denominador (income em milhares)\n",
    "    income_scale = 1000.0 if income_in_thousands else 1.0\n",
    "    denom_income = (X[\"income\"].astype(\"float64\") * income_scale + 1.0) if \"income\" in X.columns else 1.0\n",
    "\n",
    "    X[\"affordability_idx\"] = (X[\"total_spend\"].astype(\"float64\") / denom_income).astype(\"float32\")\n",
    "\n",
    "    for col in [\"longmon\", \"equipmon\", \"cardmon\", \"wiremon\"]:\n",
    "        X[f\"{col}_inc\"] = (X[col].astype(\"float64\") / denom_income).astype(\"float32\") if col in X.columns else 0.0\n",
    "\n",
    "    # 3) toxicidade\n",
    "    toxic_list = [\"internet\", \"wireless\", \"equip\", \"voice\", \"pager\"]\n",
    "    X[\"toxic_score\"] = X[[c for c in toxic_list if c in X.columns]].sum(axis=1)\n",
    "    X[\"toxic_ed\"] = (X[\"toxic_score\"].astype(\"float64\") * X[\"ed\"].astype(\"int64\")).astype(\"float32\") if \"ed\" in X.columns else 0.0\n",
    "\n",
    "    # 4) tenure em anos (tenure está em meses)\n",
    "    X[\"tenure_years\"] = (X[\"tenure\"].astype(\"float64\") / 12.0).astype(\"float32\") if \"tenure\" in X.columns else 0.0\n",
    "\n",
    "    # 5) interações de uso (age SEM /12)\n",
    "    if \"longmon\" in X.columns:\n",
    "        X[\"tenure_longmon\"] = (X[\"tenure_years\"].astype(\"float64\") * X[\"longmon\"].astype(\"float64\")).astype(\"float32\")\n",
    "        X[\"age_longmon\"]    = (X[\"age\"].astype(\"float64\") * X[\"longmon\"].astype(\"float64\")).astype(\"float32\") if \"age\" in X.columns else 0.0\n",
    "    else:\n",
    "        X[\"tenure_longmon\"] = 0.0\n",
    "        X[\"age_longmon\"] = 0.0\n",
    "\n",
    "    if \"cardmon\" in X.columns:\n",
    "        X[\"tenure_cardmon\"] = (X[\"tenure_years\"].astype(\"float64\") * X[\"cardmon\"].astype(\"float64\")).astype(\"float32\")\n",
    "        X[\"age_cardmon\"]    = (X[\"age\"].astype(\"float64\") * X[\"cardmon\"].astype(\"float64\")).astype(\"float32\") if \"age\" in X.columns else 0.0\n",
    "    else:\n",
    "        X[\"tenure_cardmon\"] = 0.0\n",
    "        X[\"age_cardmon\"] = 0.0\n",
    "\n",
    "    # 6) estabilidade (tenure_years x variáveis de perfil)\n",
    "    X[\"stability_age\"] = (X[\"tenure_years\"].astype(\"float64\") * (X[\"age\"].astype(\"float64\") - 18.0)).astype(\"float32\") if \"age\" in X.columns else 0.0\n",
    "    X[\"stability_address\"] = (X[\"tenure_years\"].astype(\"float64\") * X[\"address\"].astype(\"float64\")).astype(\"float32\") if \"address\" in X.columns else 0.0\n",
    "    X[\"stability_employ\"]  = (X[\"tenure_years\"].astype(\"float64\") * X[\"employ\"].astype(\"float64\")).astype(\"float32\") if \"employ\" in X.columns else 0.0\n",
    "\n",
    "    # 7) good_score\n",
    "    X[\"good_score\"] = X[[c for c in [\"callcard\", \"confer\", \"callwait\"] if c in X.columns]].sum(axis=1)\n",
    "\n",
    "    # limpeza final (mesma “filosofia” do arquivo do projeto: tratar inf/NaN) [file:1]\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3747277-c300-4fc1-99e3-7efcf49b1203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0495b426-1621-437a-bf6c-d11891dfdf31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configura globalmente para transformers retornarem DataFrame no transform/fit_transform\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "# Exemplo de colunas\n",
    "num_cols = [\"income\", \"tenure\"]\n",
    "cat_cols = [\"ed\", \"custcat\"]\n",
    "\n",
    "# Pipelines por tipo de dado\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "# Aplica transformações por subconjunto de colunas\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False,   # <- remove prefixos tipo \"num__\"\n",
    ").set_output(transform=\"pandas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1ef9a6-692f-45f9-afc2-0ac7e68b83d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_set_p = preprocess.fit_transform(train_set)\n",
    "train_set_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc8eaa7e-9124-41b2-a598-0a9270d91482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a88d81ee-0340-4551-a0d1-ae1e5523d27a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4d4bcee-f5e6-4e1b-b0e3-83eebd1f914b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa492efb-eda1-44a3-ac4a-41becf526a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63021ed4-9621-4fad-a4d1-d4f510be0b09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c93a501-d27e-4e3b-9b24-c7307760377c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Selecting variables for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "570dc8a0-0ced-4289-a8e1-d2c0058a69f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_set[['ENGINESIZE','FUELCONSUMPTION_COMB']]\n",
    "y_train = train_set['CO2EMISSIONS']\n",
    "\n",
    "print(f'The shape of X_train is: {X_train.shape}')\n",
    "print(f'\\nThe shape of y_train is: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f56e17-4bf5-48c8-9cb2-354101e1e358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_test = test_set[['ENGINESIZE','FUELCONSUMPTION_COMB']]\n",
    "y_test = test_set['CO2EMISSIONS']\n",
    "\n",
    "print(f'The shape of X_train is: {X_test.shape}')\n",
    "print(f'\\nThe shape of y_train is: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf69115-92e5-4034-b17c-44b3304a5b67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2faf3c-01df-40d0-8a7b-a5fdcfe30819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Note:\n",
    "---\n",
    "- For the preprocessing stage, opt for the application of the `PowerTransformer` (Yeo-Johnson method). This application, a parametric power transformation technique, evolves to **stabilize the variance** and approximate the distribution of predictors to a Normal (Gaussian) distribution. The method acts by mitigating the positive skewness (**long tail**) and correcting the **heteroscedasticity** identified in the `ENGINESIZE` variable, thus ensuring the fulfillment of the statistical predictions of the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da28d20-3b95-4bb9-bdce-50c997bb4149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --- 1. Definição das Variáveis (Feature Selection) ---\n",
    "\n",
    "# A. Variáveis para EXCLUIR (Redundantes/Derivadas)\n",
    "drop_cols = [\n",
    "    'longten', 'tollten', 'cardten', # Totais (Interação Preço x Tempo)\n",
    "    'lninc', 'loglong', 'logtoll', 'logturn', # Logs (Matemáticas)\n",
    "    'callwait', 'confer', 'ebill', # Opcionais: Se quiser simplificar o modelo (Mantenha se achar relevante)\n",
    "    'churn' # O Target jamais entra no X\n",
    "]\n",
    "\n",
    "# B. Variáveis Numéricas (Precisam de Z-Score)\n",
    "numeric_features = [\n",
    "    'tenure', 'age', 'address', 'income', 'employ', \n",
    "    'longmon', 'tollmon', 'equipmon', 'cardmon', 'wiremon'\n",
    "]\n",
    "\n",
    "# C. Variáveis Binárias (Já estão prontas: 0/1)\n",
    "binary_features = [\n",
    "    'equip', 'callcard', 'wireless', 'pager', 'internet', 'voice'\n",
    "]\n",
    "\n",
    "# D. Variáveis Categóricas/Ordinais (Precisam de Dummies)\n",
    "# 'custcat' e 'ed' são números que representam categorias\n",
    "categorical_features = ['ed', 'custcat'] \n",
    "\n",
    "# --- 2. Separação X e y ---\n",
    "\n",
    "# Garante que estamos usando apenas as colunas que sobraram após o filtro mental\n",
    "selected_features = numeric_features + binary_features + categorical_features\n",
    "\n",
    "X = df[selected_features]\n",
    "y = df['churn'] # O Target isolado\n",
    "\n",
    "# Split Estratificado (Mantém a proporção de Churn no Treino e Teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- 3. Construção do Pipeline Robusto ---\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Numéricas: Padronização\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        \n",
    "        # Categóricas: One-Hot (drop='first' remove a colinearidade)\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features),\n",
    "        \n",
    "        # Binárias: Passar direto\n",
    "        ('bin', 'passthrough', binary_features)\n",
    "    ],\n",
    "    verbose_feature_names_out=False # Mantém nomes limpos (ex: 'ed_2' em vez de 'cat__ed_2')\n",
    ")\n",
    "\n",
    "# Pipeline Final\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        solver='liblinear', # Ótimo para datasets pequenos/médios\n",
    "        penalty='l1',       # Lasso: Ajuda a zerar coeficientes inúteis (Feature Selection automático)\n",
    "        C=1.0,              # Inverso da força de regularização\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 4. Treinamento ---\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Feedback Visual\n",
    "print(\"Pipeline treinado com sucesso.\")\n",
    "print(f\"Total de Features de Entrada: {X.shape[1]}\")\n",
    "print(f\"Total de Coeficientes Gerados (após One-Hot): {len(model_pipeline.named_steps['classifier'].coef_[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea18830-3fba-4212-b838-693a270fe2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_scaler = PowerTransformer(method = 'yeo-johnson')\n",
    "y_scaler = PowerTransformer(method = 'yeo-johnson')\n",
    "\n",
    "# Train data\n",
    "X_train_preprocessed  = X_scaler.fit_transform(X_train)\n",
    "\n",
    "# Test data \n",
    "X_test_preprocessed = X_scaler.transform(X_test)\n",
    "\n",
    "# Label data\n",
    "y_train_preprocessed  = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Test data \n",
    "y_test_preprocessed = y_scaler.transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5f3773-3e73-4cac-847a-de600431f821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Train Data Preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53646bde-82f3-4d22-91f7-66e860c7a8a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_preprocessed, columns = X_scaler.get_feature_names_out(X_train.columns)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f993ea43-b75b-41da-9722-dab173e9349e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Test Data Preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b838caa-267c-492f-b657-d43acb36f91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_test_preprocessed, columns = X_scaler.get_feature_names_out(X_test.columns)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08a3d98-7ff0-43c3-b6bf-d306a44b1f8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3aab528-f9a4-4783-9ee1-9f1575635ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b53c96-92cd-4f79-81ad-05ad999ac59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create model and  K-Fold\n",
    "model = LinearRegression()\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state = 33)\n",
    "\n",
    "# Create Cross-Validation\n",
    "cv_results = cross_validate(\n",
    "    model, \n",
    "    X_train_preprocessed,\n",
    "    y_train_preprocessed,\n",
    "    cv = kfold,\n",
    "    scoring = 'r2',\n",
    "    return_estimator = True\n",
    ")\n",
    "\n",
    "# Extraction of coefficients\n",
    "coefs_list = []\n",
    "for estimator in cv_results['estimator']:\n",
    "    coefs_list.append(estimator.coef_.flatten())\n",
    "\n",
    "# Converts to a NumPy array for easier statistical analysis (Shape: [5, n_features])\n",
    "coefs_array = np.array(coefs_list)\n",
    "\n",
    "# Stability Calculation (Audit)\n",
    "coefs_mean = np.mean(coefs_array, axis = 0)\n",
    "coefs_std = np.std(coefs_array, axis = 0)\n",
    "\n",
    "# Metrics\n",
    "print(f'--- Performance Metrics ---')\n",
    "print(f'Mean R² {np.mean(cv_results['test_score']):.4f}')\n",
    "print(f'Std R²: {np.std(cv_results['test_score']):.4f}')\n",
    "\n",
    "print(f'\\n--- Stability Metrics (Coefficients) ---')\n",
    "feature_names = ['ENGINESIZE', 'FUELCONSUMPTION_COMB']\n",
    "df_stability = pd.DataFrame(\n",
    "    {\n",
    "        'Feature': feature_names,\n",
    "        'Mean Coef': coefs_mean,\n",
    "        'Std Coef': coefs_std,\n",
    "        'CV (%)': (coefs_std / np.abs(coefs_mean)) * 100\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df_stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbddd31-a65c-44a2-a174-9671fbb9f078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c3019a8-6640-4c1b-8c7e-194102db1800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_preprocessed, y_train_preprocessed)\n",
    "\n",
    "print(f'Coefficients: {model.coef_[0]}')\n",
    "print(f'Intercept: {model.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6518f4c5-da36-42cc-909b-e9dc6b36122a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e828638f-bf14-4530-9444-9ac9cecc63a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363849f1-7bc4-48ec-a5d3-3139954fce62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67991e81-aa40-4f2b-8d65-850191897f62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Bring the test Y and the predicted Y back to the \"Real World\"\n",
    "# The inverse_transform requires a 2D array, hence the reshape\n",
    "y_pred_real = y_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "y_test_real = y_scaler.inverse_transform(y_test_preprocessed)\n",
    "\n",
    "#2. Calculate metrics on the REAL scale (Grams of CO2)\n",
    "mae_real = mean_absolute_error(y_test_real, y_pred_real)\n",
    "rmse_real = root_mean_squared_error(y_test_real, y_pred_real)\n",
    "r2_real = r2_score(y_test_real, y_pred_real)\n",
    "\n",
    "print(f'--- Business Metrics (Original Scale) ---')\n",
    "print(f\"MAE Real: {mae_real:.2f} g/km\")\n",
    "print(f\"RMSE Real: {rmse_real:.2f} g/km\")\n",
    "print(f\"R2 Real: {r2_real:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Statistical Metrics (Yeo-Johnson Scale) ---\")\n",
    "print(f\"R2 Transformed: {r2_score(y_test_preprocessed, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d8dc0a-f052-496e-8a0f-e4557fa88699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Key Observations:\n",
    "---\n",
    "\n",
    "- 1. **Cross-Validation:** The application of Cross-Validation demonstrated exceptional stability in the model. The standard deviation of only **0.0170** between the k-folds confirms that the performance (average R² of **0.899**) is consistent and robust, minimizing the risk of sampling bias.\n",
    "---\n",
    "\n",
    "- 2. **Generalization Test:** In the test data, the model achieved a **Transformed R² of 0.91** (and **0.88** in the Real Scale). This transformed score, slightly higher than the training score (**0.90**), confirms that there was absolutely no *Overfitting*. The model learned the underlying physics of the data rather than memorizing noise.\n",
    "---\n",
    "- 3. **Stability (CV%):** **(`ENGINESIZE`)**: **4.05%** and **(`FUELCONSUMPTION_COMB`)**: **1.92%**. The CV is drastically below the 20% threshold, indicating \"State-of-the-Art\" stability. The **multicollinearity** (0.82 correlation) was effectively neutralized. The model assigned a clear, unwavering weight to the Fuel Consumption (Mean Coef ~0.71) as the dominant factor, while maintaining Engine Size (Mean Coef ~0.27) as a stable secondary predictor.\n",
    "---\n",
    "#### Insight:\n",
    "---\n",
    "---\n",
    "- The convergence between the training **R² (0.90)** and the transformed test **R² (0.91)** validates the **Yeo-Johnson** strategy. The slight decrease to **0.88** in the \"Real Scale\" is mathematically expected due to the non-linear inverse transformation of residuals, but it represents the honest accuracy for the business (MAE ~13g/km). Technically, the model successfully linearized a complex physical phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5163d02e-8d74-46c2-b17f-2c28dda6e442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d02198b-2650-4929-a4cb-12be0dffbc9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_lift_and_gains(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Gera a Tabela de Lift e os Gráficos de Gains/Lift.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Array com os valores reais (0 ou 1)\n",
    "        y_proba: Array com as probabilidades preditas (predict_proba)\n",
    "    \"\"\"\n",
    "    # 1. Criar DataFrame auxiliar\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_proba': y_proba})\n",
    "    \n",
    "    # 2. Ordenar por probabilidade (Ranking)\n",
    "    df = df.sort_values('y_proba', ascending=False)\n",
    "    \n",
    "    # 3. Criar os Decis (qcut divide em grupos de tamanho igual)\n",
    "    df['decile'] = pd.qcut(df['y_proba'].rank(method='first'), 10, labels=False)\n",
    "    df['decile'] = 10 - df['decile'] # Inverter para que 1 seja o Top Risk\n",
    "    \n",
    "    # 4. Agregação (A Mágica acontece aqui)\n",
    "    lift_table = df.groupby('decile')['y_true'].agg(['count', 'sum', 'mean']).reset_index()\n",
    "    lift_table.columns = ['Decile', 'Total_Customers', 'Real_Churners', 'Churn_Rate']\n",
    "    \n",
    "    # 5. Cálculos de Engenharia\n",
    "    global_churn_rate = df['y_true'].mean()\n",
    "    lift_table['Lift'] = lift_table['Churn_Rate'] / global_churn_rate\n",
    "    \n",
    "    # Cumulative Gains (Quanto do churn total eu peguei?)\n",
    "    lift_table['Cumulative_Churners'] = lift_table['Real_Churners'].cumsum()\n",
    "    lift_table['Total_Churners_In_Base'] = df['y_true'].sum()\n",
    "    lift_table['Gain_Percentage'] = lift_table['Cumulative_Churners'] / lift_table['Total_Churners_In_Base']\n",
    "    \n",
    "    # --- VISUALIZAÇÃO ---\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Gráfico de Barras (Lift por Decil)\n",
    "    sns.barplot(x='Decile', y='Lift', data=lift_table, color='skyblue', alpha=0.7, ax=ax1)\n",
    "    ax1.axhline(1, color='red', linestyle='--', label='Baseline (Aleatório)')\n",
    "    ax1.set_ylabel('Lift (x vezes melhor que aleatório)')\n",
    "    ax1.set_title('Lift Analysis & Cumulative Gains', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Gráfico de Linha (Ganho Acumulado) - Eixo Secundário\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(x=lift_table.index, y=lift_table['Gain_Percentage'], color='green', marker='o', ax=ax2, label='% Churn Capturado')\n",
    "    ax2.set_ylabel('% Total de Churners Capturados')\n",
    "    ax2.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Destaque do KPI \"Lift in Top Decile\"\n",
    "    top_decile_gain = lift_table.loc[0, 'Gain_Percentage']\n",
    "    plt.text(0, top_decile_gain, f'{top_decile_gain:.0%} Capturado', \n",
    "             bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return lift_table\n",
    "\n",
    "# Exemplo de chamada no seu notebook:\n",
    "# y_proba = model.predict_proba(X_test)[:, 1] # Pegar prob da classe 1\n",
    "# lift_df = plot_lift_and_gains(y_test, y_proba)\n",
    "# display(lift_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3635cd0c-220e-4a30-b402-804d44af36f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Flatten arrays to ensure 1D dimension.\n",
    "y_pred_real = y_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "y_test_real = y_scaler.inverse_transform(y_test_preprocessed).flatten()\n",
    "\n",
    "# Waste calculation\n",
    "residuals = y_test_real - y_pred_real\n",
    "\n",
    "# --- GRÁFICO A: Residuals vs Predicted ---\n",
    "\n",
    "# Subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize = (21, 7))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x = y_pred_real,\n",
    "    y = residuals,\n",
    "    ax = ax[0],\n",
    "    alpha = 0.6,\n",
    "    color = 'steelblue',\n",
    "    edgecolor = 'black',\n",
    "    s = 70\n",
    ")\n",
    "# Reference Line (Zero Error)\n",
    "ax[0].axhline(y = 0, color = 'crimson', linestyle = '--', linewidth = 2, label = 'Zero Error' )\n",
    "\n",
    "std_resid = np.std(residuals)\n",
    "ax[0].axhline(y = std_resid * 2, color = 'gray', linestyle = ':', alpha = 0.5, label = '+/- Std Dev')\n",
    "ax[0].axhline(y = -std_resid * 2, color = 'gray', linestyle = ':', alpha = 0.5 )\n",
    "\n",
    "ax[0].set_title('A. Homoscedasticity: Residuals vs. Predicted Values', fontsize = 14, fontweight = 'bold')\n",
    "ax[0].set_xlabel('Predicted Emission (g/km)', fontsize = 12)\n",
    "ax[0].set_ylabel('Error (Real - Predicted)', fontsize = 12)\n",
    "ax[0].legend()\n",
    "ax[0].grid(True, alpha = 0.3)\n",
    "\n",
    "\n",
    "# --- GRAPH B: Distribution of Residuals (The Normality Test)\n",
    "sns.histplot(\n",
    "    residuals, \n",
    "    kde = True, \n",
    "    ax = ax[1],\n",
    "    color = 'darkslategray',\n",
    "    edgecolor = 'black',\n",
    "   \n",
    ")\n",
    "\n",
    "mean_resid = np.mean(residuals)\n",
    "ax[1].axvline(mean_resid, color = 'gold', linestyle = '-', linewidth = 3, label = f'Mean Error: {mean_resid:.2f}')\n",
    "\n",
    "ax[1].set_title('B. Normality: Error of Distribution', fontsize = 14, fontweight = 'bold')\n",
    "ax[1].set_xlabel('Magnitude of Error (g/km)', fontsize = 12)\n",
    "ax[1].set_ylabel('Frequency', fontsize = 12)\n",
    "ax[1].legend()\n",
    "ax[1].grid(True, alpha = 0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7311148d-515d-4cfb-9759-86801eaca144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Key Observations:\n",
    "---\n",
    "\n",
    "#### 1. Technical Performance\n",
    "\n",
    "  - **Explanatory Power (Real R² Score): 0.8844**\n",
    "\n",
    "  - The model explains **88.4% of the variability** in CO2 emissions using the \"Real World\" scale (g/km). In the transformed mathematical space (Yeo-Johnson), the fit is even higher (**0.91**), confirming that the non-linear approach successfully captured the physical behavior of the data. This is a significant improvement over the simple univariate model (~0.80).\n",
    "\n",
    "  - **Margin of Error (MAE): 13.03**\n",
    "\n",
    "  - The **Mean Absolute Error** indicates that, on average, our predictions deviate by only **13.03 g/km** from the actual value. For a business context where emissions range up to 450 g/km, this represents a very high precision level (approx. 5% relative error), allowing for reliable carbon footprint estimation.\n",
    "\n",
    "  - **Sensitivity to Large Errors (RMSE vs MAE):**\n",
    "\n",
    "  * The **RMSE (20.87)** is controlled relative to the MAE (13.03). The gap of ~7.8 points is healthy. It indicates that while there are outliers (likely high-performance sports cars or heavy vehicles), the **Yeo-Johnson transformation** successfully mitigated the extreme penalties that usually distort linear models.\n",
    "---\n",
    "\n",
    "#### 2. Model Interpretation\n",
    "\n",
    "  - The model utilizes a **Power Law** approach (Yeo-Johnson) rather than a simple straight line:\n",
    "\n",
    "  - **Feature Dominance (Standardized Coefficients):**\n",
    "  - **Fuel Consumption (Coefficient ~0.71):** This is the dominant driver. The stability analysis showed a variation of only **1.92%** (CV) for this weight, proving it is the most reliable predictor.\n",
    "  - **Engine Size (Coefficient ~0.27):** This acts as a secondary adjustment factor. Even with a 0.82 correlation to fuel, the model successfully isolated its unique contribution with high stability (CV ~4.05%).\n",
    "\n",
    "  - **The \"Curved\" Surface:**\n",
    "  - Unlike a rigid linear equation, the model projects a **curved surface**. This means it understands that \"efficiency\" changes as engines get bigger. Physically, this represents the diminishing returns of combustion efficiency in larger engines, providing a much more realistic simulation than a simple linear slope.\n",
    "---\n",
    "\n",
    "#### 3. **Conclusion:**\n",
    "\n",
    "- The Multiple Regression Model with Power Transformation represents the \"State-of-the-Art\" for this dataset.\n",
    "\n",
    "- **Strengths:** - **Robustness:** The coefficient stability (CV < 5%) proves the model is immune to multicollinearity. \n",
    "- **Physical Coherence:** The residuals follow a near-perfect Gaussian distribution (Mean Bias ~0.81g), indicating that all deterministic signal has been captured.\n",
    "\n",
    "- **Limitations:**\n",
    "\n",
    "- **Interpretability:** Because the model operates in a transformed space, we cannot say \"1 liter adds X grams\" directly. We must use the inverse transformation to get real values.\n",
    "- **Scope:** The slight residual spread at the high end (>350g/km) suggests that for extreme heavy-duty vehicles, a separate model or additional features (like vehicle weight) might be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d12f08-73bb-4a99-9a37-b4df11e43014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Deployment:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18588a0f-aff5-4f4f-928b-c94ff4ae4cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DEPLOY PACKAGE: We save everything in a dictionary to ensure integrity.\n",
    "production_bundle = {\n",
    "    'model': model, \n",
    "    'pt_X': X_scaler, \n",
    "    'pt_y': y_scaler,\n",
    "}\n",
    "\n",
    "# Saved in a single \"pickle\" file\n",
    "joblib.dump(production_bundle, './artifacts/co2_pipeline_v2.pkl')\n",
    "\n",
    "# Return\n",
    "print('✅ Complete pipeline saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8c63f78-0794-4a24-b6b7-f08727102182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict_emission(engine_size, fuel_consumption):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs full inference with Yeo-Johnson pre- and post-processing.\n",
    "    \"\"\"\n",
    "    # 1. Loading (Load the Complete Package)\n",
    "    bundle = joblib.load('./artifacts/co2_pipeline_v2.pkl')\n",
    "    model_loaded = bundle['model']\n",
    "    pt_X_loaded = bundle['pt_X']\n",
    "    pt_y_loaded = bundle['pt_y']\n",
    "\n",
    "    # 2. Input Data Engineering\n",
    "    input_data = pd.DataFrame(\n",
    "        [[engine_size, fuel_consumption]],\n",
    "        columns = ['ENGINESIZE', 'FUELCONSUMPTION_COMB']\n",
    "    )\n",
    "\n",
    "    # 3. Pre-processing\n",
    "    input_transformed = pt_X_loaded.transform(input_data)\n",
    "\n",
    "    # 4. Prediction\n",
    "    prediction_transformed = model_loaded.predict(input_transformed)\n",
    "\n",
    "    # 5. Post-processing (Reducing to Grams of CO2)\n",
    "    prediction_real  = pt_y_loaded.inverse_transform(prediction_transformed.reshape(-1, 1))\n",
    "\n",
    "    result_g_km = prediction_real[0][0]\n",
    "\n",
    "    return result_g_km\n",
    "\n",
    "# --- FINAL TEST (User Acceptance Test) ---\n",
    "\n",
    "# Scenario: 2.0L car getting 8.5 L/100km\n",
    "engine = 2.0\n",
    "consumption = 8.5\n",
    "\n",
    "try:\n",
    "    prediction = predict_emission(engine, consumption)\n",
    "\n",
    "    print(f'--- INFERENCE REPORT ---')\n",
    "    print(f'Engine: {engine} L')\n",
    "    print(f'Consumption: {consumption} L/100km')\n",
    "    print(f'Predicted Emission: {prediction:.2f} g/km')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Deployment error: {e}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_Modeling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
